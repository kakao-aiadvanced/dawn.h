{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T04:31:43.847363Z",
     "start_time": "2024-06-19T04:30:30.979772Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install langchain-nomic langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python langchain-text-splitters gpt4all arxiv",
   "id": "9016526f86846c28",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-nomic\r\n",
      "  Downloading langchain_nomic-0.1.2-py3-none-any.whl (3.8 kB)\r\n",
      "Collecting langchain_community\r\n",
      "  Downloading langchain_community-0.2.5-py3-none-any.whl (2.2 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.2/2.2 MB\u001B[0m \u001B[31m4.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0mm\r\n",
      "\u001B[?25hCollecting tiktoken\r\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-macosx_10_9_x86_64.whl (961 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m961.5/961.5 kB\u001B[0m \u001B[31m9.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting langchainhub\r\n",
      "  Downloading langchainhub-0.1.20-py3-none-any.whl (5.0 kB)\r\n",
      "Collecting chromadb\r\n",
      "  Downloading chromadb-0.5.3-py3-none-any.whl (559 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m559.5/559.5 kB\u001B[0m \u001B[31m4.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting langchain\r\n",
      "  Downloading langchain-0.2.5-py3-none-any.whl (974 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m974.6/974.6 kB\u001B[0m \u001B[31m5.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting langgraph\r\n",
      "  Downloading langgraph-0.0.69-py3-none-any.whl (86 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m86.8/86.8 kB\u001B[0m \u001B[31m3.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting tavily-python\r\n",
      "  Downloading tavily_python-0.3.3-py3-none-any.whl (5.4 kB)\r\n",
      "Collecting langchain-text-splitters\r\n",
      "  Downloading langchain_text_splitters-0.2.1-py3-none-any.whl (23 kB)\r\n",
      "Collecting gpt4all\r\n",
      "  Downloading gpt4all-2.7.0-py3-none-macosx_10_15_universal2.whl (7.6 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.6/7.6 MB\u001B[0m \u001B[31m3.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting arxiv\r\n",
      "  Downloading arxiv-2.1.0-py3-none-any.whl (11 kB)\r\n",
      "Collecting nomic<4.0.0,>=3.0.29\r\n",
      "  Downloading nomic-3.0.33.tar.gz (43 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m43.9/43.9 kB\u001B[0m \u001B[31m3.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting pillow<11.0.0,>=10.3.0\r\n",
      "  Downloading pillow-10.3.0-cp310-cp310-macosx_10_10_x86_64.whl (3.5 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.5/3.5 MB\u001B[0m \u001B[31m5.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting langchain-core<0.3,>=0.1.46\r\n",
      "  Downloading langchain_core-0.2.9-py3-none-any.whl (321 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m321.8/321.8 kB\u001B[0m \u001B[31m1.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting langsmith<0.2.0,>=0.1.0\r\n",
      "  Downloading langsmith-0.1.80-py3-none-any.whl (125 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m125.3/125.3 kB\u001B[0m \u001B[31m5.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting dataclasses-json<0.7,>=0.5.7\r\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\r\n",
      "Collecting tenacity<9.0.0,>=8.1.0\r\n",
      "  Downloading tenacity-8.4.1-py3-none-any.whl (27 kB)\r\n",
      "Collecting numpy<2,>=1\r\n",
      "  Downloading numpy-1.26.4-cp310-cp310-macosx_10_9_x86_64.whl (20.6 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m20.6/20.6 MB\u001B[0m \u001B[31m7.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: requests<3,>=2 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from langchain_community) (2.32.3)\r\n",
      "Collecting SQLAlchemy<3,>=1.4\r\n",
      "  Downloading SQLAlchemy-2.0.31-cp310-cp310-macosx_10_9_x86_64.whl (2.1 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/2.1 MB\u001B[0m \u001B[31m7.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0mm\r\n",
      "\u001B[?25hCollecting aiohttp<4.0.0,>=3.8.3\r\n",
      "  Downloading aiohttp-3.9.5-cp310-cp310-macosx_10_9_x86_64.whl (400 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m400.7/400.7 kB\u001B[0m \u001B[31m12.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: PyYAML>=5.3 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from langchain_community) (6.0.1)\r\n",
      "Collecting regex>=2022.1.18\r\n",
      "  Downloading regex-2024.5.15-cp310-cp310-macosx_10_9_x86_64.whl (281 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m281.7/281.7 kB\u001B[0m \u001B[31m10.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: packaging<25,>=23.2 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from langchainhub) (24.1)\r\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2\r\n",
      "  Downloading types_requests-2.32.0.20240602-py3-none-any.whl (15 kB)\r\n",
      "Collecting kubernetes>=28.1.0\r\n",
      "  Downloading kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m5.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting pypika>=0.48.9\r\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m67.3/67.3 kB\u001B[0m \u001B[31m3.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting chroma-hnswlib==0.7.3\r\n",
      "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-macosx_10_9_x86_64.whl (219 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m219.6/219.6 kB\u001B[0m \u001B[31m4.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting opentelemetry-instrumentation-fastapi>=0.41b0\r\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.46b0-py3-none-any.whl (11 kB)\r\n",
      "Collecting pydantic>=1.9\r\n",
      "  Downloading pydantic-2.7.4-py3-none-any.whl (409 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m409.0/409.0 kB\u001B[0m \u001B[31m3.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting onnxruntime>=1.14.1\r\n",
      "  Downloading onnxruntime-1.18.0-cp310-cp310-macosx_11_0_universal2.whl (15.9 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m15.9/15.9 MB\u001B[0m \u001B[31m6.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting mmh3>=4.0.1\r\n",
      "  Downloading mmh3-4.1.0-cp310-cp310-macosx_10_9_x86_64.whl (29 kB)\r\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from chromadb) (7.7.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from chromadb) (4.12.2)\r\n",
      "Collecting tokenizers>=0.13.2\r\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-macosx_10_12_x86_64.whl (2.5 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.5/2.5 MB\u001B[0m \u001B[31m7.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting tqdm>=4.65.0\r\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.3/78.3 kB\u001B[0m \u001B[31m5.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting opentelemetry-api>=1.2.0\r\n",
      "  Downloading opentelemetry_api-1.25.0-py3-none-any.whl (59 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m59.9/59.9 kB\u001B[0m \u001B[31m7.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting orjson>=3.9.12\r\n",
      "  Downloading orjson-3.10.5-cp310-cp310-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (258 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m258.7/258.7 kB\u001B[0m \u001B[31m8.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: httpx>=0.27.0 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from chromadb) (0.27.0)\r\n",
      "Collecting grpcio>=1.58.0\r\n",
      "  Downloading grpcio-1.64.1-cp310-cp310-macosx_12_0_universal2.whl (10.3 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.3/10.3 MB\u001B[0m \u001B[31m6.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting fastapi>=0.95.2\r\n",
      "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m92.0/92.0 kB\u001B[0m \u001B[31m5.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: build>=1.0.3 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from chromadb) (1.2.1)\r\n",
      "Collecting importlib-resources\r\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\r\n",
      "Collecting posthog>=2.4.0\r\n",
      "  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m41.3/41.3 kB\u001B[0m \u001B[31m4.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\r\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.25.0-py3-none-any.whl (18 kB)\r\n",
      "Collecting typer>=0.9.0\r\n",
      "  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m47.2/47.2 kB\u001B[0m \u001B[31m3.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting bcrypt>=4.0.1\r\n",
      "  Downloading bcrypt-4.1.3-cp39-abi3-macosx_10_12_universal2.whl (506 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m506.5/506.5 kB\u001B[0m \u001B[31m6.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting uvicorn[standard]>=0.18.3\r\n",
      "  Downloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.4/62.4 kB\u001B[0m \u001B[31m4.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting opentelemetry-sdk>=1.2.0\r\n",
      "  Downloading opentelemetry_sdk-1.25.0-py3-none-any.whl (107 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m107.0/107.0 kB\u001B[0m \u001B[31m6.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting async-timeout<5.0.0,>=4.0.0\r\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\r\n",
      "Collecting requests<3,>=2\r\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.6/62.6 kB\u001B[0m \u001B[31m3.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting feedparser==6.0.10\r\n",
      "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m81.1/81.1 kB\u001B[0m \u001B[31m2.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting sgmllib3k\r\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: certifi>=2017.4.17 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (2024.6.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (2.2.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (3.3.2)\r\n",
      "Collecting multidict<7.0,>=4.5\r\n",
      "  Downloading multidict-6.0.5-cp310-cp310-macosx_10_9_x86_64.whl (30 kB)\r\n",
      "Collecting frozenlist>=1.1.1\r\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-macosx_10_9_x86_64.whl (53 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m53.8/53.8 kB\u001B[0m \u001B[31m2.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting yarl<2.0,>=1.0\r\n",
      "  Downloading yarl-1.9.4-cp310-cp310-macosx_10_9_x86_64.whl (81 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m81.2/81.2 kB\u001B[0m \u001B[31m2.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting aiosignal>=1.1.2\r\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\r\n",
      "Requirement already satisfied: tomli>=1.1.0 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (2.0.1)\r\n",
      "Requirement already satisfied: pyproject_hooks in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (1.1.0)\r\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\r\n",
      "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.2/49.2 kB\u001B[0m \u001B[31m2.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting typing-inspect<1,>=0.4.0\r\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\r\n",
      "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1\r\n",
      "  Downloading ujson-5.10.0-cp310-cp310-macosx_10_9_x86_64.whl (55 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m55.4/55.4 kB\u001B[0m \u001B[31m2.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting email_validator>=2.0.0\r\n",
      "  Downloading email_validator-2.1.2-py3-none-any.whl (30 kB)\r\n",
      "Collecting fastapi-cli>=0.0.2\r\n",
      "  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\r\n",
      "Collecting starlette<0.38.0,>=0.37.2\r\n",
      "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m71.9/71.9 kB\u001B[0m \u001B[31m3.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: jinja2>=2.11.2 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (3.1.4)\r\n",
      "Collecting python-multipart>=0.0.7\r\n",
      "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\r\n",
      "Requirement already satisfied: httpcore==1.* in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.0.5)\r\n",
      "Requirement already satisfied: anyio in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (4.4.0)\r\n",
      "Requirement already satisfied: sniffio in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.3.1)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\r\n",
      "Collecting oauthlib>=3.2.2\r\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m151.7/151.7 kB\u001B[0m \u001B[31m4.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting requests-oauthlib\r\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\r\n",
      "Collecting google-auth>=1.0.1\r\n",
      "  Downloading google_auth-2.30.0-py2.py3-none-any.whl (193 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m193.7/193.7 kB\u001B[0m \u001B[31m4.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\r\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\r\n",
      "Collecting jsonpatch<2.0,>=1.33\r\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\r\n",
      "Collecting pandas\r\n",
      "  Downloading pandas-2.2.2-cp310-cp310-macosx_10_9_x86_64.whl (12.6 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.6/12.6 MB\u001B[0m \u001B[31m4.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting loguru\r\n",
      "  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.5/62.5 kB\u001B[0m \u001B[31m3.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting pyarrow\r\n",
      "  Downloading pyarrow-16.1.0-cp310-cp310-macosx_10_15_x86_64.whl (28.3 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m28.3/28.3 MB\u001B[0m \u001B[31m5.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting pyjwt\r\n",
      "  Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\r\n",
      "Collecting jsonlines\r\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\r\n",
      "Collecting click\r\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m97.9/97.9 kB\u001B[0m \u001B[31m5.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting rich\r\n",
      "  Downloading rich-13.7.1-py3-none-any.whl (240 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m240.7/240.7 kB\u001B[0m \u001B[31m7.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting coloredlogs\r\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m46.0/46.0 kB\u001B[0m \u001B[31m5.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting sympy\r\n",
      "  Downloading sympy-1.12.1-py3-none-any.whl (5.7 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.7/5.7 MB\u001B[0m \u001B[31m5.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting protobuf\r\n",
      "  Downloading protobuf-5.27.1-cp38-abi3-macosx_10_9_universal2.whl (412 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m412.3/412.3 kB\u001B[0m \u001B[31m5.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting flatbuffers\r\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\r\n",
      "Collecting deprecated>=1.2.6\r\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\r\n",
      "Requirement already satisfied: importlib-metadata<=7.1,>=6.0 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.1.0)\r\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.25.0\r\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.25.0-py3-none-any.whl (17 kB)\r\n",
      "Collecting opentelemetry-proto==1.25.0\r\n",
      "  Downloading opentelemetry_proto-1.25.0-py3-none-any.whl (52 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m52.5/52.5 kB\u001B[0m \u001B[31m3.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting googleapis-common-protos~=1.52\r\n",
      "  Downloading googleapis_common_protos-1.63.1-py2.py3-none-any.whl (229 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m229.2/229.2 kB\u001B[0m \u001B[31m6.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting protobuf\r\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m394.2/394.2 kB\u001B[0m \u001B[31m7.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting opentelemetry-semantic-conventions==0.46b0\r\n",
      "  Downloading opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl (130 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m130.5/130.5 kB\u001B[0m \u001B[31m6.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting opentelemetry-instrumentation-asgi==0.46b0\r\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.46b0-py3-none-any.whl (14 kB)\r\n",
      "Collecting opentelemetry-instrumentation==0.46b0\r\n",
      "  Downloading opentelemetry_instrumentation-0.46b0-py3-none-any.whl (29 kB)\r\n",
      "Collecting opentelemetry-util-http==0.46b0\r\n",
      "  Downloading opentelemetry_util_http-0.46b0-py3-none-any.whl (6.9 kB)\r\n",
      "Collecting wrapt<2.0.0,>=1.0.0\r\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-macosx_10_9_x86_64.whl (37 kB)\r\n",
      "Requirement already satisfied: setuptools>=16.0 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (65.5.0)\r\n",
      "Collecting asgiref~=3.0\r\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\r\n",
      "Collecting monotonic>=1.5\r\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\r\n",
      "Collecting backoff>=1.10.0\r\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\r\n",
      "Collecting annotated-types>=0.4.0\r\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\r\n",
      "Collecting pydantic-core==2.18.4\r\n",
      "  Downloading pydantic_core-2.18.4-cp310-cp310-macosx_10_12_x86_64.whl (1.9 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.9/1.9 MB\u001B[0m \u001B[31m5.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting greenlet!=0.4.17\r\n",
      "  Downloading greenlet-3.0.3-cp310-cp310-macosx_11_0_universal2.whl (270 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m270.1/270.1 kB\u001B[0m \u001B[31m5.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting huggingface-hub<1.0,>=0.16.4\r\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m402.6/402.6 kB\u001B[0m \u001B[31m6.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: shellingham>=1.3.0 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\r\n",
      "Collecting watchfiles>=0.13\r\n",
      "  Downloading watchfiles-0.22.0-cp310-cp310-macosx_10_12_x86_64.whl (394 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m395.0/395.0 kB\u001B[0m \u001B[31m6.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting httptools>=0.5.0\r\n",
      "  Downloading httptools-0.6.1-cp310-cp310-macosx_10_9_x86_64.whl (77 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m77.6/77.6 kB\u001B[0m \u001B[31m4.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting websockets>=10.4\r\n",
      "  Downloading websockets-12.0-cp310-cp310-macosx_10_9_x86_64.whl (121 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m121.3/121.3 kB\u001B[0m \u001B[31m6.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0\r\n",
      "  Downloading uvloop-0.19.0-cp310-cp310-macosx_10_9_x86_64.whl (793 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m793.9/793.9 kB\u001B[0m \u001B[31m6.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting python-dotenv>=0.13\r\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\r\n",
      "Collecting dnspython>=2.0.0\r\n",
      "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m307.7/307.7 kB\u001B[0m \u001B[31m5.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting pyasn1-modules>=0.2.1\r\n",
      "  Downloading pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m181.2/181.2 kB\u001B[0m \u001B[31m6.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting rsa<5,>=3.1.4\r\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\r\n",
      "Collecting cachetools<6.0,>=2.0.0\r\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\r\n",
      "Collecting fsspec>=2023.5.0\r\n",
      "  Downloading fsspec-2024.6.0-py3-none-any.whl (176 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m176.9/176.9 kB\u001B[0m \u001B[31m5.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: filelock in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.15.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from importlib-metadata<=7.1,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.19.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from jinja2>=2.11.2->fastapi>=0.95.2->chromadb) (2.1.5)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.46->langchain-nomic) (3.0.0)\r\n",
      "Collecting markdown-it-py>=2.2.0\r\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m87.5/87.5 kB\u001B[0m \u001B[31m5.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from rich->nomic<4.0.0,>=3.0.29->langchain-nomic) (2.18.0)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.1)\r\n",
      "Collecting mypy-extensions>=0.3.0\r\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\r\n",
      "Collecting humanfriendly>=9.1\r\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m86.8/86.8 kB\u001B[0m \u001B[31m5.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting pytz>=2020.1\r\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m505.5/505.5 kB\u001B[0m \u001B[31m6.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting tzdata>=2022.7\r\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m345.4/345.4 kB\u001B[0m \u001B[31m5.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting mpmath<1.4.0,>=1.1.0\r\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m536.2/536.2 kB\u001B[0m \u001B[31m4.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting mdurl~=0.1\r\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\r\n",
      "Collecting pyasn1<0.7.0,>=0.4.6\r\n",
      "  Downloading pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m85.3/85.3 kB\u001B[0m \u001B[31m3.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hBuilding wheels for collected packages: nomic, pypika\r\n",
      "  Building wheel for nomic (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for nomic: filename=nomic-3.0.33-py3-none-any.whl size=44365 sha256=c5b4b5c49faa0c904734b425c9574fefe66f096b5a0db3cbbaa332f890c704dd\r\n",
      "  Stored in directory: /Users/dawn-h/Library/Caches/pip/wheels/ce/8c/96/d2fabef908400092401af7287fab4dfca8eb65dca496490831\r\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53724 sha256=40b37fd1737c47899917b9ec5ad2efecce76cddd76dd73500eceedab778f0d40\r\n",
      "  Stored in directory: /Users/dawn-h/Library/Caches/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\r\n",
      "Successfully built nomic pypika\r\n",
      "Installing collected packages: sgmllib3k, pytz, pypika, mpmath, monotonic, mmh3, flatbuffers, wrapt, websockets, uvloop, ujson, tzdata, types-requests, tqdm, tenacity, sympy, requests, regex, python-multipart, python-dotenv, pyjwt, pydantic-core, pyasn1, protobuf, pillow, orjson, opentelemetry-util-http, oauthlib, numpy, mypy-extensions, multidict, mdurl, marshmallow, loguru, jsonpatch, jsonlines, importlib-resources, humanfriendly, httptools, grpcio, greenlet, fsspec, frozenlist, feedparser, dnspython, click, cachetools, bcrypt, backoff, async-timeout, asgiref, annotated-types, yarl, watchfiles, uvicorn, typing-inspect, tiktoken, starlette, SQLAlchemy, rsa, requests-oauthlib, pydantic, pyasn1-modules, pyarrow, posthog, pandas, opentelemetry-proto, markdown-it-py, langchainhub, huggingface-hub, gpt4all, googleapis-common-protos, email_validator, deprecated, coloredlogs, chroma-hnswlib, arxiv, aiosignal, tokenizers, tavily-python, rich, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, onnxruntime, langsmith, google-auth, dataclasses-json, aiohttp, typer, opentelemetry-semantic-conventions, opentelemetry-instrumentation, nomic, langchain-core, kubernetes, opentelemetry-sdk, opentelemetry-instrumentation-asgi, langgraph, langchain-text-splitters, langchain-nomic, fastapi-cli, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, langchain, fastapi, langchain_community, chromadb\r\n",
      "\u001B[33m  DEPRECATION: sgmllib3k is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001B[0m\u001B[33m\r\n",
      "\u001B[0m  Running setup.py install for sgmllib3k ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Attempting uninstall: requests\r\n",
      "    Found existing installation: requests 2.32.3\r\n",
      "    Uninstalling requests-2.32.3:\r\n",
      "      Successfully uninstalled requests-2.32.3\r\n",
      "Successfully installed SQLAlchemy-2.0.31 aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.7.0 arxiv-2.1.0 asgiref-3.8.1 async-timeout-4.0.3 backoff-2.2.1 bcrypt-4.1.3 cachetools-5.3.3 chroma-hnswlib-0.7.3 chromadb-0.5.3 click-8.1.7 coloredlogs-15.0.1 dataclasses-json-0.6.7 deprecated-1.2.14 dnspython-2.6.1 email_validator-2.1.2 fastapi-0.111.0 fastapi-cli-0.0.4 feedparser-6.0.10 flatbuffers-24.3.25 frozenlist-1.4.1 fsspec-2024.6.0 google-auth-2.30.0 googleapis-common-protos-1.63.1 gpt4all-2.7.0 greenlet-3.0.3 grpcio-1.64.1 httptools-0.6.1 huggingface-hub-0.23.4 humanfriendly-10.0 importlib-resources-6.4.0 jsonlines-4.0.0 jsonpatch-1.33 kubernetes-30.1.0 langchain-0.2.5 langchain-core-0.2.9 langchain-nomic-0.1.2 langchain-text-splitters-0.2.1 langchain_community-0.2.5 langchainhub-0.1.20 langgraph-0.0.69 langsmith-0.1.80 loguru-0.7.2 markdown-it-py-3.0.0 marshmallow-3.21.3 mdurl-0.1.2 mmh3-4.1.0 monotonic-1.6 mpmath-1.3.0 multidict-6.0.5 mypy-extensions-1.0.0 nomic-3.0.33 numpy-1.26.4 oauthlib-3.2.2 onnxruntime-1.18.0 opentelemetry-api-1.25.0 opentelemetry-exporter-otlp-proto-common-1.25.0 opentelemetry-exporter-otlp-proto-grpc-1.25.0 opentelemetry-instrumentation-0.46b0 opentelemetry-instrumentation-asgi-0.46b0 opentelemetry-instrumentation-fastapi-0.46b0 opentelemetry-proto-1.25.0 opentelemetry-sdk-1.25.0 opentelemetry-semantic-conventions-0.46b0 opentelemetry-util-http-0.46b0 orjson-3.10.5 pandas-2.2.2 pillow-10.3.0 posthog-3.5.0 protobuf-4.25.3 pyarrow-16.1.0 pyasn1-0.6.0 pyasn1-modules-0.4.0 pydantic-2.7.4 pydantic-core-2.18.4 pyjwt-2.8.0 pypika-0.48.9 python-dotenv-1.0.1 python-multipart-0.0.9 pytz-2024.1 regex-2024.5.15 requests-2.31.0 requests-oauthlib-2.0.0 rich-13.7.1 rsa-4.9 sgmllib3k-1.0.0 starlette-0.37.2 sympy-1.12.1 tavily-python-0.3.3 tenacity-8.4.1 tiktoken-0.7.0 tokenizers-0.19.1 tqdm-4.66.4 typer-0.12.3 types-requests-2.32.0.20240602 typing-inspect-0.9.0 tzdata-2024.1 ujson-5.10.0 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-12.0 wrapt-1.16.0 yarl-1.9.4\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T04:37:45.128899Z",
     "start_time": "2024-06-19T04:37:43.068853Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install langchain-openai",
   "id": "87d0446f080ad128",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-openai\r\n",
      "  Downloading langchain_openai-0.1.8-py3-none-any.whl (38 kB)\r\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from langchain-openai) (0.7.0)\r\n",
      "Collecting openai<2.0.0,>=1.26.0\r\n",
      "  Downloading openai-1.34.0-py3-none-any.whl (325 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m325.5/325.5 kB\u001B[0m \u001B[31m17.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: langchain-core<0.3,>=0.2.2 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from langchain-openai) (0.2.9)\r\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2.2->langchain-openai) (24.1)\r\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2.2->langchain-openai) (0.1.80)\r\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2.2->langchain-openai) (8.4.1)\r\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2.2->langchain-openai) (1.33)\r\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2.2->langchain-openai) (2.7.4)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2.2->langchain-openai) (6.0.1)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from openai<2.0.0,>=1.26.0->langchain-openai) (0.27.0)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from openai<2.0.0,>=1.26.0->langchain-openai) (4.12.2)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from openai<2.0.0,>=1.26.0->langchain-openai) (4.4.0)\r\n",
      "Collecting distro<2,>=1.7.0\r\n",
      "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\r\n",
      "Requirement already satisfied: sniffio in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from openai<2.0.0,>=1.26.0->langchain-openai) (1.3.1)\r\n",
      "Requirement already satisfied: tqdm>4 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from openai<2.0.0,>=1.26.0->langchain-openai) (4.66.4)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.5.15)\r\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.31.0)\r\n",
      "Requirement already satisfied: idna>=2.8 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.26.0->langchain-openai) (3.7)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.26.0->langchain-openai) (1.2.1)\r\n",
      "Requirement already satisfied: httpcore==1.* in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain-openai) (1.0.5)\r\n",
      "Requirement already satisfied: certifi in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain-openai) (2024.6.2)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain-openai) (0.14.0)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.2->langchain-openai) (3.0.0)\r\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.2->langchain-openai) (3.10.5)\r\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain-openai) (2.18.4)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain-openai) (0.7.0)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.2.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dawn-h/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.3.2)\r\n",
      "Installing collected packages: distro, openai, langchain-openai\r\n",
      "Successfully installed distro-1.9.0 langchain-openai-0.1.8 openai-1.34.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T04:38:27.033286Z",
     "start_time": "2024-06-19T04:38:27.030120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# Set the API key\n",
    "os.environ['OPENAI_API_KEY'] = \"\""
   ],
   "id": "563f323c15bd6ec",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T04:39:54.327298Z",
     "start_time": "2024-06-19T04:39:54.324228Z"
    }
   },
   "cell_type": "code",
   "source": "local_llm = 'llama3'",
   "id": "4dd38ded4a654ddd",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T04:38:35.551130Z",
     "start_time": "2024-06-19T04:38:28.499562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Index\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T04:40:08.477585Z",
     "start_time": "2024-06-19T04:40:00.008098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Router\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an expert at routing a\n",
    "    user question to a vectorstore or web search. Use the vectorstore for questions on LLM  agents,\n",
    "    prompt engineering, and adversarial attacks. You do not need to be stringent with the keywords\n",
    "    in the question related to these topics. Otherwise, use web-search. Give a binary choice 'web_search'\n",
    "    or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and\n",
    "    no premable or explanation. Question to route: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "question_router = prompt | llm | JsonOutputParser()\n",
    "question = \"llm agent memory\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(question_router.invoke({\"question\": question}))"
   ],
   "id": "71cf68d02559cefe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datasource': 'vectorstore'}\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:33:00.696137Z",
     "start_time": "2024-06-19T06:32:57.009135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Retrieval Grader -> relevance Checker\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance\n",
    "    of a retrieved document to a user question. If the document contains keywords related to the user question,\n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"agent memory\"   \n",
    "docs = retriever.invoke(question)\n",
    "print(docs[1].metadata[\"source\"])\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ],
   "id": "ac58df29496cee0d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://lilianweng.github.io/posts/2023-06-23-agent/\n",
      "{'score': 'yes'}\n"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T04:40:45.294813Z",
     "start_time": "2024-06-19T04:40:40.723240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Generate -> Generate Answer\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks.\n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
    "    Use three sentences maximum and keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ],
   "id": "c1396f6993ceba49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context mentions \"Memory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\" This suggests that the agent's memory is a long-term memory module that stores experiences and information in a natural language format.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T04:40:58.418658Z",
     "start_time": "2024-06-19T04:40:55.153432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Hallucination Grader\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether\n",
    "    an answer is grounded in / supported by a set of facts. Give a binary 'yes' or 'no' score to indicate\n",
    "    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a\n",
    "    single key 'score' and no preamble or explanation. <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here are the facts:\n",
    "    \\n ------- \\n\n",
    "    {documents}\n",
    "    \\n ------- \\n\n",
    "    Here is the answer: {generation}  <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"generation\", \"documents\"],\n",
    ")\n",
    "\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ],
   "id": "703d88fa94d262e3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T04:41:02.740384Z",
     "start_time": "2024-06-19T04:41:02.128861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Answer Grader\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether an\n",
    "    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is\n",
    "    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|> Here is the answer:\n",
    "    \\n ------- \\n\n",
    "    {generation}\n",
    "    \\n ------- \\n\n",
    "    Here is the question: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "\n",
    "answer_grader = prompt | llm | JsonOutputParser()\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ],
   "id": "263346f6ec712f1a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T05:52:48.455550Z",
     "start_time": "2024-06-19T05:52:46.642209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Academic Grader\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an academic reviewer assessing whether a question is grounded in /\n",
    "    supported by a set of facts. Provide a binary ‘yes’ or ‘no’ score to indicate whether the question is grounded in / supported by a set of facts. Provide\n",
    "    the binary score as a JSON with a single key ‘score’ and no preamble or explanation. \n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here are the question:\n",
    "    \\n ------- \\n\n",
    "    {question}\n",
    "    \\n ------- \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "academic_grader = prompt | llm | JsonOutputParser()\n",
    "academic_grader.invoke({\"question\": question})"
   ],
   "id": "3b6246867218a771",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'no'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T05:53:24.749103Z",
     "start_time": "2024-06-19T05:53:24.351047Z"
    }
   },
   "cell_type": "code",
   "source": "academic_grader.invoke({\"question\": \"What is the deep learning system?\"})",
   "id": "35ea70c1d8e51e87",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:55:45.226930Z",
     "start_time": "2024-06-19T06:55:42.663714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Summary Generator\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a summarizer whose task is to condense and distill the essential points from a given text. Your goal is to provide a concise and clear summary that captures the main ideas and key details. user Here is the text to summarize\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|> \n",
    "    Here is the answer:\n",
    "    \\n ------- \\n\n",
    "    {documents}\n",
    "    \\n ------- \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"documents\"],\n",
    ")\n",
    "\n",
    "summerizer = prompt | llm | StrOutputParser()\n",
    "summerizer.invoke({\"documents\": doc_txt})"
   ],
   "id": "7a82200c9d4b1385",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is a concise summary of the text:\\n\\nA Large Language Model (LLM) can be used as the core controller for building autonomous agents that can solve complex problems. The LLM-powered agent system consists of several key components, including:\\n\\n* Planning: breaking down large tasks into smaller subgoals and refining past actions to improve future results.\\n* Memory: storing information learned from experiences and using it to inform decision-making.\\n\\nThe potential benefits of this approach include the ability to efficiently handle complex tasks, learn from mistakes, and produce high-quality results.'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T04:47:48.349936Z",
     "start_time": "2024-06-19T04:47:48.342026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tavily import TavilyClient\n",
    "tavily = TavilyClient(api_key='')"
   ],
   "id": "e36d72753f267eb",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T06:06:51.998755Z",
     "start_time": "2024-06-19T06:06:48.959067Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install pymupdf",
   "id": "66fef53e4b1f271f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\r\n",
      "  Downloading PyMuPDF-1.24.5-cp310-none-macosx_10_9_x86_64.whl (3.2 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.2/3.2 MB\u001B[0m \u001B[31m43.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting PyMuPDFb==1.24.3\r\n",
      "  Downloading PyMuPDFb-1.24.3-py3-none-macosx_10_9_x86_64.whl (15.3 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m15.3/15.3 MB\u001B[0m \u001B[31m80.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: PyMuPDFb, pymupdf\r\n",
      "Successfully installed PyMuPDFb-1.24.3 pymupdf-1.24.5\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T07:48:29.401101Z",
     "start_time": "2024-06-19T07:48:26.917756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "docs = ArxivLoader(query=question, load_max_docs=2).load()\n",
    "arxiv_results = \"\\n\".join([d.page_content for d in docs])\n",
    "print(arxiv_results.page_content)\n",
    "arxiv_results = Document(page_content=arxiv_results)"
   ],
   "id": "b9b17cb7f1eacc49",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[123], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m docs \u001B[38;5;241m=\u001B[39m ArxivLoader(query\u001B[38;5;241m=\u001B[39mquestion, load_max_docs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mload()\n\u001B[1;32m      5\u001B[0m arxiv_results \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([d\u001B[38;5;241m.\u001B[39mpage_content \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m docs])\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43marxiv_results\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpage_content\u001B[49m)\n\u001B[1;32m      7\u001B[0m arxiv_results \u001B[38;5;241m=\u001B[39m Document(page_content\u001B[38;5;241m=\u001B[39marxiv_results)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'str' object has no attribute 'page_content'"
     ]
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T08:09:05.322186Z",
     "start_time": "2024-06-19T08:09:05.299248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pprint import pprint\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "### State\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add search\n",
    "        documents: list of documents\n",
    "        hallucination: whether hallucination is present \n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]\n",
    "    academic: str\n",
    "\n",
    "\n",
    "### Nodes\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    \n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "def summerize_documents(state):\n",
    "    \"\"\"\n",
    "    Summerize the retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---SUMMERIZE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Summerize\n",
    "    generation = summerizer.invoke({\"documents\": documents})\n",
    "    return {\"documents\": generation, \"question\": question}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    If any document is not relevant, we will set a flag to run web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        \n",
    "        grade = score[\"score\"]\n",
    "        \n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "\n",
    "\n",
    "def grade_academic(state):\n",
    "    \"\"\"\n",
    "    Determines whether the question is academic or not\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ACADEMIC CHECK---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Academic check\n",
    "    score = academic_grader.invoke({\"question\": question})\n",
    "    grade = score[\"score\"]\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: ACADEMIC QUESTION---\")\n",
    "        return {\"documents\": documents, \"question\": question, \"academic\": \"Yes\"}\n",
    "    else:\n",
    "        print(\"---DECISION: NON-ACADEMIC QUESTION---\")\n",
    "        return {\"documents\": documents, \"question\": question, \"academic\": \"No\"}\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = tavily.search(query=question)['results']\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    if documents is not None:\n",
    "        documents.append(web_results)\n",
    "    else:\n",
    "        documents = [web_results]\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def arxiv_search(state):\n",
    "    \"\"\"\n",
    "    Arxiv search based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended arxiv results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ARXIV SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Arxiv search\n",
    "    docs = ArxivLoader(query=question, load_max_docs=2).load()\n",
    "    arxiv_results = \"\\n\".join([d.page_content for d in docs])\n",
    "    arxiv_results = Document(page_content=arxiv_results)\n",
    "    if documents is not None:\n",
    "        documents.append(arxiv_results)\n",
    "    else:\n",
    "        documents = [arxiv_results]\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "\n",
    "def format_output(state):\n",
    "    \"\"\"\n",
    "    Format the output for the user\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Formatted output\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---FORMAT OUTPUT---\")\n",
    "    generation = state[\"generation\"] + \"\\n\\n Reference documents: \\n\" + \"\\n\".join(\n",
    "        [d.metadata[\"source\"] for d in state[\"documents\"]]\n",
    "    )\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "### Edges\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    web_search = state[\"web_search\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\"\n",
    "        )\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "### Conditional edge\n",
    "\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = score[\"score\"]\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score[\"score\"]\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\"\n",
    "\n",
    "\n",
    "def route_to_search(state):\n",
    "    \"\"\"\n",
    "    Determines whether to route to web search or arxiv search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE TO SEARCH---\")\n",
    "    academic = state[\"academic\"]\n",
    "\n",
    "    if academic == \"Yes\":\n",
    "        print(\"---DECISION: ACADEMIC SEARCH---\")\n",
    "        return \"arxiv\"\n",
    "    else:\n",
    "        print(\"---DECISION: WEB SEARCH---\")\n",
    "        return \"web_search\"\n"
   ],
   "id": "3cf60236bd6e972",
   "outputs": [],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T08:09:05.924315Z",
     "start_time": "2024-06-19T08:09:05.916631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"websearch\", web_search)  # web search\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"format_output\", format_output)  # generatae final answer\n",
    "workflow.add_node(\"academic_checker\", grade_academic)  # route to search\n",
    "workflow.add_node(\"arxiv_search\", arxiv_search)  # arxiv search\n",
    "\n",
    "# Build graph\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"websearch\": \"academic_checker\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"academic_checker\",\n",
    "    route_to_search,\n",
    "    {\n",
    "        \"web_search\": \"websearch\",\n",
    "        \"arxiv\": \"arxiv_search\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"arxiv_search\", \"grade_documents\")\n",
    "workflow.add_edge(\"websearch\", \"grade_documents\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"useful\": \"format_output\",\n",
    "        \"not useful\": \"generate\",\n",
    "        \"not supported\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"format_output\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ],
   "id": "14c444e5b0b89687",
   "outputs": [],
   "execution_count": 126
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T08:09:21.253134Z",
     "start_time": "2024-06-19T08:09:07.005641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test\n",
    "inputs = {\"question\": \"What are the types of agent memory?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "print(value[\"generation\"])"
   ],
   "id": "8a501bd0af952606",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "'Finished running: retrieve:'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "'Finished running: grade_documents:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "'Finished running: generate:'\n",
      "---FORMAT OUTPUT---\n",
      "'Finished running: format_output:'\n",
      "The types of agent memory mentioned in the context are:\n",
      "\n",
      "1. Sensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc.) after the original stimuli have ended.\n",
      "2. Short-term memory: This provides the agent with the capability to learn and recall information over a short period.\n",
      "3. Long-term memory: This allows the agent to retain and recall information over extended periods, often by leveraging an external vector store and fast retrieval.\n",
      "\n",
      "Note that these types of memory are mentioned in the context as being used in LLM-powered autonomous agents.\n",
      "\n",
      " Reference documents: \n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/\n"
     ]
    }
   ],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T07:44:09.249662Z",
     "start_time": "2024-06-19T07:43:49.329273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = {\"question\": \"What is the Deep learning system?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "print(value[\"generation\"])"
   ],
   "id": "9e93d1271bc05f27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "'Finished running: retrieve:'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "Human-in-the-loop adversarial generation, proposed by Wallace et al. (2019) , aims to build toolings to guide humans to break models. They experimented with QuizBowl QA dataset and designed an adversarial writing interface for humans to write similar Jeopardy style questions to trick the model to make wrong predictions. Each word is highlighted in different colors according to its word importance (i.e. change in model prediction probability upon the removal of the word). The word importance is approximated by the gradient of the model w.r.t. the word embedding.\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\n",
      "\n",
      "Planning\n",
      "\n",
      "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
      "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
      "\n",
      "\n",
      "Memory\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\n",
      "[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\n",
      "[12] Parisi et al. “TALM: Tool Augmented Language Models”\n",
      "[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\n",
      "[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\n",
      "[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "Fig. 1. Overview of a LLM-powered autonomous agent system.\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\n",
      "'Finished running: grade_documents:'\n",
      "---ACADEMIC CHECK---\n",
      "---DECISION: ACADEMIC QUESTION---\n",
      "---ROUTE TO SEARCH---\n",
      "---DECISION: ACADEMIC SEARCH---\n",
      "'Finished running: academic_checker:'\n",
      "---ARXIV SEARCH---\n",
      "'Finished running: arxiv_search:'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\n",
      "\n",
      "Planning\n",
      "\n",
      "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
      "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
      "\n",
      "\n",
      "Memory\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\n",
      "[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\n",
      "[12] Parisi et al. “TALM: Tool Augmented Language Models”\n",
      "[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\n",
      "[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\n",
      "[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "Deep Machine Learning Based Egyptian Vehicle License \n",
      "Plate Recognition Systems \n",
      "Mohamed Shehata \n",
      "Systems & Computers Engineering \n",
      "Department, Faculty of Engineering \n",
      "Al-Azhar University, Egypt \n",
      "+2 0122 113 6789 \n",
      "m-shehata@azhar.edu.eg \n",
      "Mohamed Taha Abou-Kreisha \n",
      "Mathematical & Computer Science \n",
      "Department, Faculty of Science, Al-\n",
      "Azhar University. Egypt. \n",
      "+2 0100 252 8607 \n",
      "drkresha@gmail.com \n",
      "Hany Elnashar \n",
      "Project Development Unite, Faculty of \n",
      "Computers and Artificial Intelligent, \n",
      "Beni-Suef University  \n",
      "+2 0100 657 4960 \n",
      "hsoliman@fcis.bsu.edu.eg \n",
      " \n",
      " \n",
      "ABSTRACT \n",
      "Automated Vehicle License Plate (VLP) detection and recognition \n",
      "have ended up being a significant research issue as of late. VLP \n",
      "localization and recognition are some of the most essential \n",
      "techniques for managing traffic using digital techniques. In this \n",
      "paper, four smart systems are developed to recognize Egyptian \n",
      "vehicles’ license plates. Two systems are based on character \n",
      "recognition, which are (System1: Characters Recognition with \n",
      "Classical \n",
      "Machine \n",
      "Learning) \n",
      "and \n",
      "(System2: \n",
      "Characters \n",
      "Recognition with Deep Machine Learning). The other two \n",
      "systems are based on the whole plate recognition which are \n",
      "(System3: Whole License Plate Recognition with Classical \n",
      "Machine Learning) and (System4: Whole License Plate \n",
      "Recognition with Deep Machine Learning). We use object \n",
      "detection algorithms, and machine learning based object \n",
      "recognition algorithms. The performance of the developed \n",
      "systems has been tested on real images, and the experimental \n",
      "results demonstrate that the best detection accuracy rate for VLP \n",
      "is provided by using the deep learning method. Where the VLP \n",
      "detection accuracy rate is better than the classical system by 32%. \n",
      "However, the best detection accuracy rate for Vehicle License \n",
      "Plate Arabic Character (VLPAC) is provided by using the \n",
      "classical method. Where VLPAC detection accuracy rate is better \n",
      "than the deep learning-based system by 6%. Also, the results show \n",
      "that deep learning is better than the classical technique used in \n",
      "VLP recognition processes. Where the recognition accuracy rate \n",
      "is better than the classical system by 8%.  Finally, the paper \n",
      "output recommends a robust VLP recognition system based on \n",
      "both statistical and deep machine learning. \n",
      "Keywords \n",
      "Artificial Intelligent; Computer Vision; Deep Learning; Object \n",
      "Detection; Object Recognition. \n",
      "1. INTRODUCTION \n",
      "Vehicle License Plates Recognition (VLPR) system was invented \n",
      "in 1976 at the Police Scientific Development Branch in the UK \n",
      "but only in the late 90s became an important application domain \n",
      "of pattern recognition [1]. It plays a very important part in an \n",
      "intelligent transportation system (ITS) [2], and also plays a role in \n",
      "the decrease in the number of traffic violations, resulting in safer \n",
      "traffic flow. It's utilized to identify vehicles based only on their \n",
      "number plate. Since every car or truck has a special number plate, \n",
      "no further external cards, tags, or transmitters have to be \n",
      "recognized. It may be used in different road and security traffic \n",
      "management applications, like road electronic toll collection, \n",
      "average speed enforcement, parking control devices, detection of \n",
      "stolen cars, etc [3]. LPR system is an image processing as well as \n",
      "pattern recognition approach utilized to recognize vehicle license \n",
      "plate characters through a photo of a vehicle. \n",
      "The paper is structured as follows: Section two delivers a briefing \n",
      "of relevant works. An overview of the complete proposed \n",
      "solutions is explained in section three. And also, the paper is \n",
      "concluded with Section five. \n",
      "69\n",
      "2. RELATED WORKS \n",
      "In this section, a survey on automatic VLP detection and \n",
      "recognition techniques based on computer vision is presented. It \n",
      "reviews the implementation of new technologies in both the plate \n",
      "detection and character identification aspects of the process. All \n",
      "methods considered are from researches published in 2016 \n",
      "onwards. \n",
      "Shaimaa Ahmed Elsaid, et al. [4], build a Real Time License \n",
      "Plates Recognition system that put on to Saudi Arabian VLPs. \n",
      "This method recognizes equally Arabic along with Indian \n",
      "numerals and also Latin alphabets and limited Arabic. The \n",
      "developed structure locates Saudi license plates in a shot picture \n",
      "no matter the time of day or license plate dimensions. It is \n",
      "composed of 5 primary stages; preprocessing, license plate \n",
      "localization, character segmentation, features extraction, along \n",
      "with character recognition utilizing Optical Character Recognition \n",
      "(OCR). To display the effectiveness of the proposed method, it \n",
      "was tested on 470 LP pictures taken in outdoor atmospheres such \n",
      "as numerous kinds of vehicles with various shadow, skewness and \n",
      "noise effects. The experimental results yield 96% segmentation \n",
      "accuracy as well as 94.7% recognition accuracy. \n",
      "Rhen Anjerome Bedruz, et al. [5], proposed a vehicle plate optical \n",
      "character recognition technique using SIFT integrated fuzzy logic \n",
      "and image segmentation. Picture segmentation separates every \n",
      "single character in a plate region to get the features of any \n",
      "character obtained. Scale Invariant Feature Transform or even \n",
      "SIFT, on the other hand, allows for every feature of every \n",
      "character from the plate. Fuzzy reason analyzes the features from \n",
      "the SIFT algorithm that is suggested to identify the characters \n",
      "properly. This system used MATLAB to identify the functionality \n",
      "of the algorithm. the proposed algorithm helped remove plate \n",
      "character features along with knowing the characters in a certain \n",
      "picture. Results indicate the algorithm has an accuracy of 90.75%. \n",
      "Ohnmar Khin, et al.  [6], applied and also attempted to verify \n",
      "VLPR for Myanmar automobile quantity plates utilizing \n",
      "localization as well as recognition processes put on to license \n",
      "plates. Threshold based technique and bounding box techniques \n",
      "were used in various stages of the recognition process. This was \n",
      "tested for forty-eight letters as well as fifty-two numbers and also \n",
      "resulted in an accuracy rate of 90%. \n",
      "Tejendra Panchal, et al. [7], addressed VLP limitation along with \n",
      "the integrated division technique. Various frameworks are \n",
      "provided for tag acknowledgment, each one bearing its specific \n",
      "purposes of blocks and intrigue. The serious stride of the VLPR \n",
      "framework is the actual repression of the number plate, \n",
      "Segmentation, Recognition. Harris corner computation is \n",
      "suggested to this method which ends up being effective in \n",
      "changing motion and enlightened lighting problems. Even though \n",
      "the exactness of VLP confinement is nourished forward to the \n",
      "segmentation organize. The Segmentation is enhanced by a \n",
      "method for associated segment investigation solidified with Pixel \n",
      "verify, Aspect proportion as well as Height of characters. The \n",
      "reenacted outcomes are coming out as well as the general \n",
      "accuracy was 93.84 %. \n",
      "Jia Wang, et al. [8], developed a system for VLP detection and \n",
      "recognition called secondary positioning. It's used on the New \n",
      "Zealand license plate also it's dependent on Hue Saturation Value \n",
      "(HSV) color space. The first phase is founding the white light \n",
      "areas within the HSV color area, and after that, the precise \n",
      "location of the plate number is detected by learning the vertical \n",
      "edge of the plate number. For the knowing step, a template \n",
      "matching is applied. It contains a correction coefficient that's \n",
      "calculated between the templates as well as testing pictures. \n",
      "Accuracies, from plate number localization as well as the \n",
      "recognition, are above 75 % and 70 % respectively. \n",
      "Hana M. Alyahya [9], designed a method that recognizes the plate \n",
      "number effectively. The bilinear interpolation algorithm was \n",
      "utilized to resize the license plate picture to get a fixed dimension \n",
      "of most license plates. Right after resizing the image, various \n",
      "preprocessing methods were applied to eliminate the needless \n",
      "parts. Lastly, an OCR which is grounded on the ANN classifier is \n",
      "utilized to recognize the characters. MATLAB device was used \n",
      "for developing and testing. Many experiments were conducted to \n",
      "compute accuracy and clearly show the effectiveness of the design \n",
      "system. The results showed that the system has 92 % accuracy and \n",
      "will recognize both Arabic and English letters and numbers. \n",
      "Table 1 summariness the results of the mentioned related works. \n",
      "Table 1. Related Works VLP Recognition Systems Results \n",
      "VLP Recognition Method \n",
      "Accuracy \n",
      "Hue Saturation Value [8] \n",
      "0.75 VLP \n",
      "Threshold & bounding box [6] \n",
      "0.90 VLP \n",
      "Fuzzy logic [5] \n",
      "0.91 VLP \n",
      "ANN [9] \n",
      "0.92 VLP \n",
      "Optical Character Recognition [4] \n",
      "0.94 VLP \n",
      "Harris corner [7] \n",
      "0.94 VLP \n",
      "3. PROPOSED SYSTEMS \n",
      "VLPR systems have an impressive spread both for their practical \n",
      "application and interest as a research issue. This section presents \n",
      "VLP detection and recognition systems based on computer vision \n",
      "techniques, which consists of foreground object extraction, VLP \n",
      "detection methods, VLP classification.  \n",
      "We identify Egyptian vehicle license plates instantly by four \n",
      "developed systems, making use of both deep and classical \n",
      "machine learning methods for license plate characters as well as a \n",
      "whole license plate. The input of these systems is a photo of a car \n",
      "that contains the license plate taken by a 2 MP (1920 × 1080) \n",
      "pixels resolution camera. Egyptian vehicle license plate styles are \n",
      "illustrated in Fig. 1.  \n",
      " \n",
      " \n",
      "Whereby the proposed systems assume the following: \n",
      "• \n",
      "The plates have a rectangular shape. \n",
      "• \n",
      "The width to height relationship of the license plate is known \n",
      "in advance. \n",
      "Figure 1. Samples of Egyptian VLP design. \n",
      "70\n",
      "• \n",
      "The plate has black characters on a bright white background \n",
      "• \n",
      "The metal vertical bar separates the plate into two groups of \n",
      "characters. \n",
      "• \n",
      "Items present in the plate center could be ignored, as they're \n",
      "unnecessary for the license plate recognition operation. \n",
      "• \n",
      "The colored horizontal bars above the character's location \n",
      "might be ignored \n",
      "These systems will be discussed in details in the following lines. \n",
      "Fig. 2, shows the model of the overall system. \n",
      " \n",
      " \n",
      "Two methods are developed for VLP detection and two methods \n",
      "are developed for VLPAC detection. The four methods are based \n",
      "on both statistical and deep learning techniques. \n",
      "3.1 VLP Detection Methods \n",
      "The primary phase begins with VLP extraction, and that is the \n",
      "primary key stage in VLPR systems. The goal of this particular \n",
      "stage is producing a VLP region as shown in Fig. 3. \n",
      " \n",
      " \n",
      "We developed two methods for VLP detection. One of them is \n",
      "based on statistical techniques and the other one is based on deep \n",
      "learning techniques. \n",
      "3.1.1 VLP detection based on a statistical technique \n",
      "To get the rectangular shape which has the identical attributes as \n",
      "the license plate; the algorithm initially converts the input color \n",
      "image to greyscale, then a Prewitt edge detector [10] is applied \n",
      "that returns a binary image of the same dimensions as the input \n",
      "image, then a dilation process is performed for those horizontal \n",
      "and vertical lines detected to avoid broken lines. And then, the \n",
      "algorithm discards minor lines that are not likely to be a \n",
      "component of the plate borders using a label connected \n",
      "components technique. Afterward, the algorithm checks for just \n",
      "about any horizontal lines linking two vertical lines which \n",
      "represent the vertical boundaries of the number plate, taking into \n",
      "account the license plate height to width ratio. \n",
      "Out of the previous section, we can compute the VLP height and \n",
      "width, additionally, we can identify the upper left corner of the \n",
      "VLP, although it still not enough for the extraction process, as \n",
      "parts of some characters might be lost as an outcome of non-\n",
      "alignment of the license plate with the horizontal axis as shown in \n",
      "Fig. 4. \n",
      " \n",
      " \n",
      "This might happen because of the camera as well as angle, defects \n",
      "in the fixation of the plate, or deformities of the plate as an \n",
      "outcome of traffic accidents. \n",
      "After the plate is located, the alignment algorithm starts the \n",
      "preparation for the following stage. The whole idea depends upon \n",
      "the detection of the horizontal line within the license plate, \n",
      "demonstrated in Fig. 5.  \n",
      " \n",
      " \n",
      "To detect that line we followed the next steps. The Horizontal \n",
      "Prewitt edge detected lines, which represent the top, bottom, along \n",
      "with tiny lines are discarded, as shown in Fig. 6. \n",
      " \n",
      "Next a novel rotation algorithm uses to compute the necessary \n",
      "rotation angle for the successful alignment of the horizontal line \n",
      "within the license plate with the horizontal axis. The rotation \n",
      "algorithm rotates the input image around its center stepwise by \n",
      "angles from 10 to -10, in steps of 0.5. At each step, the algorithm \n",
      "calculates the height difference between the upper left corner and \n",
      "the upper right corner of VLP, as shown in Fig. 7.  \n",
      " \n",
      "Figure 2. Overall Systems Model. \n",
      " \n",
      "Figure 3. VLP Detection Input & Output. \n",
      "Figure 4. Examples of License Plate Alignments. \n",
      " \n",
      "Figure 5. The Horizontal Line Inside the License Plate. \n",
      " \n",
      "Figure 6. Horizontal Prewitt Edge Detection. \n",
      "71\n",
      " \n",
      "The minimum difference recorded, represents the required \n",
      "rotation angle; finally, we can extract the VLP region utilizing the \n",
      "resulting angle. \n",
      "A number of 1726 VLPs have been tested. Table 2 shows the \n",
      "confusion matrix and the results of that method. \n",
      "Table 2. VLP Detection Based on Edge Detector Results \n",
      "FN \n",
      "FP \n",
      "TP \n",
      "Precision \n",
      "Recall \n",
      "Accuracy \n",
      "519 \n",
      "274 \n",
      "1207 \n",
      "0.82 \n",
      "0.70 \n",
      "0.60 \n",
      "3.1.2 VLP detection based on deep learning \n",
      "technique \n",
      "Faster region deep convolutional neural network (Faster R-\n",
      "DCNN) detector [11] is used to find candidate bounding boxes. \n",
      "To locate VLP.  \n",
      "Faster R-DCNN has two networks, Region Proposal Network \n",
      "(RPN) for generating region proposals and a Deep Convolutional \n",
      "Neural Network (DCNN) using these proposals to detect objects. \n",
      "RPN ranks region boxes called anchors and proposes the ones \n",
      "most likely containing objects. \n",
      "Anchors play an important role in Faster R-DCNN. An anchor is a \n",
      "box. In the default configuration of Faster R-DCNN, there are 9 \n",
      "anchors at a position of an image. Fig. 8, shows 9 anchors shapes. \n",
      " \n",
      " \n",
      " \n",
      "The input image is (1920, 1080) and we choose one position at \n",
      "every stride of 32, there will be 2040 (60x34) positions. This leads \n",
      "to 18360 (2040 x 9) boxes to consider. The sheer size is hardly \n",
      "smaller than the combination of the sliding window and pyramid. \n",
      "Our designed network does not need to consider the square boxes. \n",
      "For that, we reduce the number of anchors to be 6 to increase the \n",
      "speed of the system. \n",
      "The output of RPN is a bunch of boxes that will be examined by a \n",
      "classifier and regressor to eventually check the occurrence of \n",
      "vehicles, VLP, or VLPC. As the RPN predicts the possibility of an \n",
      "anchor being background or foreground, and refine the anchor. \n",
      "The first step of training a classifier is to make a training dataset. \n",
      "The training data is that the anchors we get from the above \n",
      "process and therefore the ground-truth boxes. The problem we'd \n",
      "like to unravel here is how we use the ground-truth boxes to label \n",
      "the anchors. The basic idea here is that we would like to label the \n",
      "anchors having the upper overlaps with ground-truth boxes as \n",
      "foreground, those with lower overlaps as background. It needs \n",
      "some tweaks and compromise to separate the foreground and \n",
      "background. Now we have labels for the anchors. \n",
      "Let’s say the 1920X1080 image shrinks 32 times to a 60x34 \n",
      "feature map after applying DCNNs. Every position in the feature \n",
      "map has 6 anchors, and every anchor has two possible labels \n",
      "(background, foreground). If we make the depth of the feature \n",
      "map 12 (6 anchors x 2 labels), we will make every anchor have a \n",
      "vector with two values representing the foreground and \n",
      "background. If we feed these vectors into a SoftMax logistic \n",
      "regression activation function, it will predict the labels. Now the \n",
      "training data is complete with features and labels. \n",
      "In Faster R-DCNN, receptive fields of various anchors often \n",
      "overlap one another, as you'll from the above graph. It leaves the \n",
      "RPN to be position-aware. After RPN, we get proposed regions \n",
      "with different sizes. a special sized region means different sized \n",
      "DCNN feature maps. It’s tough to form an efficient structure to \n",
      "figure on features of various sizes. Region of Interest (ROI) \n",
      "Pooling can simplify the matter by reducing the feature maps to \n",
      "an equivalent size. Unlike Max-Pooling which features a fixed \n",
      "size, ROI Pooling splits the input feature map into a hard and fast \n",
      "number (k) of roughly equal regions, then apply Max-Pooling on \n",
      "every region. Therefore, the output of ROI Pooling is usually (k) \n",
      "no matter the dimensions of the input. \n",
      "At last, an R-DCNN takes input from both the DCNN feature \n",
      "network and RPN and generates the ultimate class and bounding \n",
      "box. R-DCNN consists of 4 fully connected, two of them stacked \n",
      "common layers shared by a classification layer and a bounding \n",
      "box regression layer, to assist it to classify only the within of the \n",
      "bounding boxes, the features are cropped consistent with the \n",
      "bounding boxes, to get the detected bounding boxes for the VLP. \n",
      "Fig. 9 [11], summarizes the Faster R-DCNN process. \n",
      " \n",
      " \n",
      " \n",
      "We used 5000 Egyptian VLPs images taken from different angles \n",
      "of view with different types and sizes. We did some augmented \n",
      "effects on the VLPs images as we rotate them with different \n",
      "angles and deform them a little bit, which leads to train a Faster \n",
      "R-DCNN object detector by 100000 original and augmented \n",
      "Figure 7. License Plate Alignments Process. \n",
      " \n",
      "Figure 8. Faster R-DCNN Anchor Boxes. \n",
      " \n",
      "Figure 9. Faster R-DCNN Process. \n",
      " \n",
      "72\n",
      "VLPs images, for detecting a VLP. Table 3 shows the confusion \n",
      "matrix and the results of that method. \n",
      "Table 3. VLP Detection Based on Faster R-DCNN Results \n",
      "FN \n",
      "FP \n",
      "TP \n",
      "Precision \n",
      "Recall \n",
      "Accuracy \n",
      "186 \n",
      "162 \n",
      "3859 \n",
      "0.96 \n",
      "0.95 \n",
      "0.92 \n",
      "3.2 VLPAC Detection Methods \n",
      "We developed two robust methods based on statistical and deep \n",
      "learning techniques, that aim to extract the Arabic letters and \n",
      "Hindi digits within VLP, where the models’ input is the VLP \n",
      "image which the previous system considered. And the system \n",
      "output should be the boundary boxes of the VLPAC. As shown in \n",
      "Fig. 10. \n",
      " \n",
      " \n",
      "3.2.1 VLPAC detection based on a statistical \n",
      "technique \n",
      "This particular stage input would be the VLP that the previous \n",
      "method considered, and tries to find Arabic letters as well as \n",
      "Hindi digits inside these regions. To speed up the procedure we \n",
      "resized the input image to a lower resolution sample. \n",
      "Before any processing or segmentation on a VLP image may be \n",
      "done, it's really helpful to convert the image from a grey scale or \n",
      "color to a white and black bitmap. This's a straightforward \n",
      "procedure that entails just scanning the image and changing each \n",
      "pixel value to either white or black based on if they're above or \n",
      "below a certain threshold. Otsu's method [12] was used to \n",
      "compute this particular threshold amount value. \n",
      "When there are any ripples on any character such as a mark over \n",
      "character, the character will appear as artificially segmented. As a \n",
      "workaround, a dilation procedure follows in both axes. Utilizing \n",
      "the Connected Components Label (CCL) method [13] to count \n",
      "objects within separate character regions. Small objects like \n",
      "ripples as well as English characters in the bottom part of the \n",
      "characters region could subsequently be recognized utilizing ratio \n",
      "checks between the height and the width of the VLP. Small \n",
      "objects are then discarded to boost the effectiveness of the \n",
      "recognition process. To restore these lost areas after obtaining the \n",
      "required characters, we gradually expanded the capture area \n",
      "around every recognized character in all directions as shown in \n",
      "Fig. 11.  \n",
      " \n",
      "Then each character is going to be cropped into a block that \n",
      "contains no extra white spaces in all of the four sides of the \n",
      "characters. \n",
      "A number of 7434 VLPACs have been tested. Table 4 shows the \n",
      "confusion matrix and the results of that method. \n",
      "Table 4. VLPAC Detection Based on CCL Results \n",
      "FN \n",
      "FP \n",
      "TP \n",
      "Precision \n",
      "Recall \n",
      "Accuracy \n",
      "196 \n",
      "124 \n",
      "7238 \n",
      "0.98 \n",
      "0.97 \n",
      "0.96 \n",
      "3.2.2 VLPAC detection based on deep learning \n",
      "Faster R-DCNN detector which is mentioned in Section 3.1.2 is \n",
      "used to find candidate bounding boxes. To locate VLPAC. But \n",
      "here the designed network does not need to consider the \n",
      "horizontal anchor boxes and need the square boxes. For that, we \n",
      "reduce the number of anchors to be 6 to increase the speed of the \n",
      "system. \n",
      "We used 13500 Egyptian VLPACs images taken from different \n",
      "5000 VLP with different types and sizes. We did some augmented \n",
      "effects on the VLPACs images as we rotate them with different \n",
      "angles. Plus, we made a little deformation on the characters' \n",
      "images, which leads to train a Faster R-DCNN object detector by \n",
      "270000 original and augmented VLPACs images, for detecting a \n",
      "VLPACs. A number of new 24714 VLPACs belongs to 4045 VLP \n",
      "have been tested. Table 5 shows the confusion matrix and the \n",
      "results of that method. \n",
      "Table 5. VLPAC Detection Based on Faster R-DCNN Results \n",
      "FN \n",
      "FP \n",
      "TP \n",
      "Precision \n",
      "Recall \n",
      "Accuracy \n",
      "1805 \n",
      "784 \n",
      "22609 \n",
      "0.96 \n",
      "0.92 \n",
      "0.90 \n",
      "3.3 System 1: Characters Recognition with \n",
      "Classical Machine Learning \n",
      "This system depends on a sequential, multistage image processing \n",
      "method for the License Plate Extraction as well as Characters \n",
      "Segmentation. as shown in Fig. 12. \n",
      " \n",
      " \n",
      "This system input is the VLPACs that were detected before. The \n",
      "system output is the characters' names as shown in Fig. 13. \n",
      " \n",
      " \n",
      "As feature extraction is the most remarkable part of a recognition \n",
      "system that has a significant impact on the recognition \n",
      "performance, we use Principal Component Analysis (PCA) for \n",
      "feature extraction [14]. \n",
      "The PCA may be a popular dimensionality reduction technique, \n",
      "PCA has been used widely in computer vision applications. it's a \n",
      "ubiquitous statistical procedure for unsupervised feature \n",
      "extraction, data analysis, and compression. it's a really extensive \n",
      "literature, a summary of which may be found in several \n",
      "monographs. Normally in PCA, the target function τ for choosing \n",
      "new directions is defined by \n",
      "                      ( )  \n",
      "    \n",
      "     \n",
      " \n",
      " Eq. 1 \n",
      "Where \n",
      "              *(   * +)(   * +) +    Eq. 2 \n",
      "Is the sample covariance matrix. Here   denotes the mean value. \n",
      "The following proposition asserts a well-known property of the \n",
      "Figure 10. VLPAC Detection Input & Output. \n",
      "Figure 11. VLPAC Detection. \n",
      "Figure 12. The workflow of VLPR System Based on \n",
      "Character Recognition. \n",
      " \n",
      "Two, Three, Eight, Geem, Taa, \n",
      "Haa \n",
      "Figure 13. VLPR System Based on Character \n",
      "Recognition Input & Output. \n",
      "73\n",
      "first equation, that defines  ( ) as the variance of the centralized \n",
      "sample vectors \n",
      "    * +       * + \n",
      "Projected onto vector \n",
      " \n",
      "‖ ‖  \n",
      "Hence, this method prefers directions that have an outsized \n",
      "variance. Fig. 14, shows the effect of PCA on 2D data sets, the \n",
      "synthetic data sets on the right-hand side were transformed using \n",
      "PCA and therefore the results are shown on the left-hand side. \n",
      "Besides the info sets themselves, we also plotted their distribution \n",
      "along the axes. \n",
      " \n",
      " \n",
      "A dataset of characters used to train the PCA feature extraction \n",
      "function. A sample of that data set is shown in Fig. 15. \n",
      " \n",
      " \n",
      "Then the features extracted from the input character images are \n",
      "compared to the features extracted from characters images in a \n",
      "training database, and the best similarity is considered. To \n",
      "measure the similarity, a minimum distance classifier K-NN \n",
      "Euclidean distance [15] is used. K- NN are often computed \n",
      "through computing the space between the test image and therefore \n",
      "the training images, a matrix during which each image is \n",
      "represented as a vector. then find the minimum distance between \n",
      "the test image and therefore the training images. the worth of the \n",
      "minimum distance decides the group of the test image. On the \n",
      "opposite hand, the minimum distance classifier is often computed \n",
      "by calculating the minimum distance vector. The classifier is \n",
      "based on many distance measures. After arranging the feature \n",
      "vectors of the training images into a matrix   of   column vectors \n",
      "           , the distance D between a given test feature vector \n",
      "     and each of the training feature vectors    is calculated using \n",
      "various distance metrics. Euclidean Distance is one among the \n",
      "foremost commonly used distance measures. It is the square root \n",
      "of the sum of the squared distances of two vector values (    ,   ). \n",
      "The Euclidean distance are often seen because the shortest \n",
      "distance between two points. it's defined as: \n",
      "         √(       )(       )   \n",
      "     Eq. 3 \n",
      "Another metric is the standardized Euclidean distance. It is \n",
      "defined as [118]: \n",
      "        √(       )   (       )        Eq. 4 \n",
      "Where     is the diagonal matrix with diagonal elements given \n",
      "by   \n",
      " , and   \n",
      "   is the variance of the         element of  . The \n",
      "advantage of the Standardized Euclidean metric is that it takes \n",
      "into account the variance mentioned above. This distance is \n",
      "widely used in 2D metrics but can also be highly useful in n-\n",
      "dimensional vectors such as feature vectors due to their low \n",
      "computational costs. \n",
      "10 images for 9 digits and 17 alphabets with a total number of 260 \n",
      "characters have been used for the training process, and a new 300 \n",
      "VLP contains 1873 characters have been tested. The system \n",
      "precision is 0.85, recall is 0.84, and accuracy is 0.97 for character \n",
      "recognition, which leads to a VLP recognition accuracy rate of \n",
      "0.89. \n",
      "3.4 System 2: Characters Recognition with \n",
      "Deep Learning \n",
      "This system utilized the identical workflow of (System 1) as \n",
      "shown in Fig. 12, besides it's used a full DCNN [16] instead of the \n",
      "DCT method that is needed for feature extraction as well as a \n",
      "KNN classifier. \n",
      "In the field of deep learning, DCNN has performance that is \n",
      "excellent for visual recognition. In this system, DCNN AlexNet. \n",
      "The DCNN basic architecture is shown in Fig. 16 [17]. \n",
      " \n",
      " \n",
      "The DCNN first layer defined the sort also because the size of the \n",
      "input character image; next, we defined the center layers of the \n",
      "system. the center layers are comprised of repetitive blocks of \n",
      "convolution layers, which may be liable for executing the image \n",
      "manipulation procedures with the convolution functions to control \n",
      "also as extract the character image features. Every convolution \n",
      "layer is followed by a cross channel normalization layer, a \n",
      "Rectified Linear Measure (ReLU), alongside a pooling layer to \n",
      "rework the convolution process results and also create the DCNN \n",
      "invariant to image translation and illumination. These layers make \n",
      "the core building blocks of DCNN. The last layers are the \n",
      "classification layers and have 26 classes which represent all \n",
      "characters utilized in the Egyptian car place. For the classification \n",
      "process, the input size must be the standard size of the training \n",
      "images. Two fully connected neural network layers are used. \n",
      "These layers combine all the features learned by the previous \n",
      "layers across the image to acknowledge the larger patterns. \n",
      "The proposed network utilized to train a database consisted of \n",
      "270000 photos for license plate characters which were used before \n",
      "in section 3.2.2. The New 300 VLP contains 1873 characters that \n",
      "have been tested. The system precision is 0.95, recall is 0.95, and \n",
      "Figure 14. PCA Effect on 2D Data Sets. \n",
      " \n",
      "Figure 15. Samples of Characters Trained by PCA. \n",
      " \n",
      "Figure 16. DCNN Basic Architecture. \n",
      "74\n",
      "accuracy is 0.99 for character recognition, which leads to a VLP \n",
      "recognition accuracy rate of 95%. \n",
      "3.5 System \n",
      "3: \n",
      "Whole \n",
      "License \n",
      "Plate \n",
      "Recognition with Classical Machine Learning \n",
      "In this system, we excluded the character segmentation stage that \n",
      "had been utilized before in the previous two recognition systems, \n",
      "as shown in Fig. 17. Right after, extracting the VLP, as we \n",
      "immediately recognize it, using a feature extraction algorithm, \n",
      "along with classical machine learning based classification \n",
      "algorithms. \n",
      " \n",
      "Whereas we followed the same image processing steps, that are \n",
      "pointed out in the VLP detection stage until the VLP extraction \n",
      "takes place. as we do not need a process for character \n",
      "segmentation. Rather, we found features in the whole VLP image. \n",
      "Where the system input is the VLP image and the system output is \n",
      "the recognized VLP as shown in Fig. 18. \n",
      " \n",
      " \n",
      "After detecting the VLP as illustrated before, we directly extract it \n",
      "and start to recognize it, using a feature extraction algorithm, and \n",
      "classical machine learning based classification algorithms. \n",
      "We don't need the character segmentation process. Instead, we \n",
      "found features in the whole VLP image   (   ). Each image is \n",
      "      . DCT features extraction technique is used, by dividing \n",
      "the input image into equally sized non-overlapped blocks 8x8 \n",
      "pixels. Each block is transformed from the spatial domain to the \n",
      "frequency domain and represented by its Discrete Cosine \n",
      "Transform (DCT) coefficients [18]. \n",
      "        ∑\n",
      "∑\n",
      " (   )   \n",
      " (    ) \n",
      "   \n",
      "    \n",
      "   \n",
      "    \n",
      "   \n",
      "   \n",
      " (    ) \n",
      "   \n",
      "   Eq. 5 \n",
      "At           and           \n",
      "where   (   )  Input image,     Input image rows number, and \n",
      "    Input image columns number. \n",
      "   {\n",
      " \n",
      "√                                       \n",
      "√\n",
      " \n",
      "                          \n",
      "          Eq. 6 \n",
      "    {\n",
      " \n",
      "√                                      \n",
      "√\n",
      " \n",
      "                          \n",
      "     Eq. 7 \n",
      "From the remaining DCT coefficients, the ones containing the \n",
      "highest information are extracted via zigzag scan [18], with the \n",
      "most visually significant information being concentrated in the \n",
      "first few DCT coefficients ordered in a zigzag pattern from the top \n",
      "left corner as shown in Fig. 19. \n",
      " \n",
      " \n",
      "Information concentration in the top left corner is due to the \n",
      "correlation between image and DCT properties. For images, the \n",
      "most visually significant frequencies are of low-mid range, with \n",
      "higher frequencies giving finer details of the image. For the 2D \n",
      "DCT, coefficients correspond to sinusoids of frequencies \n",
      "increasing from left to right and top to bottom. Therefore, those in \n",
      "the upper corner are the coefficients associated with the lowest \n",
      "frequencies. The first coefficient, known as the DCT coefficient, \n",
      "represents the average intensity of the block and is the most \n",
      "affected by variation in illumination. DCT features have been \n",
      "used in a holistic appearance-based sense, or local appearance-\n",
      "based sense that ignores spatial information during the \n",
      "classification step [19]. \n",
      "The DCT coefficients obtained from each block are concatenated \n",
      "to construct the feature vector that is used by the classifier. The \n",
      "previous process is performed on all the training images. To \n",
      "identify unknown images the following steps will be applied. \n",
      "After that, the features extracted from the input VLP images are \n",
      "compared to the features extracted from a set of VLP in a training \n",
      "database, and the best similarity is considered by using the SVM \n",
      "classifier [20].  \n",
      "SVM is an unsupervised approach based on statistical learning \n",
      "theory. It estimates the optimal boundary in the feature space by \n",
      "combining a maximal margin strategy with a kernel method; this \n",
      "process is called a kernel machine. The machine is trained \n",
      "according to the structural risk minimization criterion. The \n",
      "decision boundaries are directly derived from the training data set \n",
      "by learning. The SVM maps the inputs into a high-dimensional \n",
      "feature space through a selected kernel function. Then, it \n",
      "constructs an optimal separating hyper-plane in the feature space. \n",
      "The dimensionality of the feature space is determined by the \n",
      "number of support vectors extracted from the training data shown \n",
      "in Fig. 20.  \n",
      " \n",
      " \n",
      "Figure 17. LPR system based on whole license plate \n",
      "recognition. \n",
      " \n",
      "Plate No. which recorded in database \n",
      "as:  \n",
      "(Eight, Four, Three, Taa, Noon, Seen) \n",
      "Figure 18. VLPR System Input & Output. \n",
      "Figure 19. Zigzag Sequence. \n",
      "Figure 20. Optimal Boundary Searched by SVM. \n",
      "75\n",
      "The SVM can locate all the support vectors, which exclusively \n",
      "determine \n",
      "the \n",
      "decision \n",
      "boundaries. \n",
      "To \n",
      "estimate \n",
      "the \n",
      "misclassification rate (risk), the so-called leave-one-out procedure \n",
      "is used. It removes one of    training samples, perform training \n",
      "using the remaining training samples and tests the removed \n",
      "sample with the newly derived hyper-plane. It repeats this process \n",
      "for all of the samples, and the total number of errors becomes the \n",
      "estimation of the risk. \n",
      "A number of 500 VLPs images from various angles of views, for \n",
      "100 different vehicles, used for training. And new 100 VLPs have \n",
      "been tested. The system precision is 0.85, recall is 0.76, and \n",
      "accuracy is 0.90. \n",
      "3.6 System \n",
      "4: \n",
      "Whole \n",
      "License \n",
      "Plate \n",
      "Recognition with Deep Learning \n",
      "This system utilized the same workflow illustrated in (System3) \n",
      "as shown in Fig. 17, where, we excluded the character \n",
      "segmentation stage that had been utilized before. Right after, \n",
      "extracting the VLP, as we immediately recognize it, using a \n",
      "DCNN which mentioned in (System 2) for feature extraction, and \n",
      "two fully connected neural network layers as a classifier, to \n",
      "recognize the license plate. \n",
      "We used 5000 VLP images of different types and sizes. We did \n",
      "some augmented effects on the VLP images as we rotate them \n",
      "with different angles. Plus, we made a little deformation on the \n",
      "VLP images, which leads to train a DCNN by 100000 original and \n",
      "augmented VLPs images, divided into 5000 classes. A number of \n",
      "new 1000 VLP have been tested. The system precision is 0.91, \n",
      "recall is 0.89, and accuracy is 0.93. \n",
      "4. CONCLUSIONS \n",
      "We've talked about the concepts of VLPR in the Egyptian \n",
      "environment. We proposed two methods for VLP detection and \n",
      "two methods for VLPAC detection, based on statistical and deep \n",
      "learning methods. Then we created four comprehensive smart \n",
      "systems to identify as well as recognize Egyptian license plates. \n",
      "Finally, we conclude that the most accurate VLPR proposed \n",
      "system which has the highest accuracy. It should use Faster R-\n",
      "DNN deep learning method for VLP extraction followed by CCL \n",
      "statistical method for VLPAC detection. At last, a DCNN is \n",
      "utilized for character recognition prosses. \n",
      "5. REFERENCES \n",
      "[1] Oluchi, I.B., et al., Development of a Nigeria Vehicle \n",
      "License Plate Detection System. Applications of Modelling \n",
      "and Simulation, 2019. 3(3): p. 188-195.  \n",
      "[2] Pandey, V., A Survey of License Plate Localization. Journal \n",
      "of the Gujarat Research Society, 2019. 21(17): p. 621-630.  \n",
      "[3] Yao, L., et al. Research and Application of License Plate \n",
      "Recognition Technology Based on Deep Learning. in Journal \n",
      "of Physics: Conference Series. 2019. IOP Publishing. \n",
      "[4] Bedruz, R.A., et al. Fuzzy logic based vehicular plate \n",
      "character recognition system using image segmentation and \n",
      "scale-invariant feature transform. in 2016 IEEE region 10 \n",
      "conference (TENCON). 2016. IEEE. \n",
      "[5] Sannella, M. J. 1994. Constraint Satisfaction and Debugging \n",
      "for Interactive User Interfaces. Doctoral Thesis. UMI Order \n",
      "Number: UMI Order No. GAX95-09398., University of \n",
      "Washington.  \n",
      "[6] Khin, O., M. Phothisonothai, and S. Choomchuay. Myanmar \n",
      "character extraction from vehicle images using aspect ratio \n",
      "and bounding box. in 2018 International Workshop on \n",
      "Advanced Image Technology (IWAIT). 2018. IEEE.  \n",
      "[7] Panchal, T., H. Patel, and A. Panchal, License plate detection \n",
      "using Harris corner and character segmentation by integrated \n",
      "approach from an image. Procedia Computer Science, 2016. \n",
      "79: p. 419-425. \n",
      "[8] Wang, J., B. Bacic, and W.Q. Yan, An effective method for \n",
      "plate \n",
      "number \n",
      "recognition. \n",
      "Multimedia \n",
      "Tools \n",
      "and \n",
      "Applications, 2018. 77(2): p. 1679-1692. \n",
      "[9] Alyahya, H.M., et al. Saudi license plate recognition system \n",
      "using \n",
      "artificial \n",
      "neural \n",
      "network \n",
      "classifier. \n",
      "in \n",
      "2017 \n",
      "International Conference on Computer and Applications \n",
      "(ICCA). 2017. IEEE. \n",
      "[10] Ahmed, A.S., Comparative study among Sobel, Prewitt and \n",
      "Canny edge detection operators used in image processing. J. \n",
      "Theor. Appl. Inf. Technol, 2018. 96(19): p. 6517-6525. \n",
      " \n",
      "[11] Jiang, H. and E. Learned-Miller. Face Detection with the \n",
      "Faster R-CNN. in 2017 12th IEEE International Conference \n",
      "on Automatic Face & Gesture Recognition (FG 2017). 2017. \n",
      "[12] Goh, T.Y., et al., Performance analysis of image \n",
      "thresholding: Otsu technique. Measurement, 2018. 114: p. \n",
      "298-307. \n",
      " \n",
      "[13] Korn, M., D. Sanders, and J. Pauli. Moving Object Detection \n",
      "by Connected Component Labeling of Point Cloud \n",
      "Registration Outliers on the GPU. in VISIGRAPP (6: \n",
      "VISAPP). 2017. \n",
      "[14] Abikoye, O.C., I.F. Shoyemi, and T.O. Aro, Comparative \n",
      "analysis of illumination normalizations on principal \n",
      "component analysis based feature extraction for face \n",
      "recognition. \n",
      "FUOYE \n",
      "Journal \n",
      "of \n",
      "Engineering \n",
      "and \n",
      "Technology, 2019. 4(1): p. 67-69. \n",
      "[15] Mondal, S. and S. Bag. Face recognition using PCA and \n",
      "minimum distance classifier. in Proceedings of the 5th \n",
      "International Conference on Frontiers in Intelligent \n",
      "Computing: Theory and Applications. 2017. Springer. \n",
      "[16] Albawi, \n",
      "S., \n",
      "T.A. \n",
      "Mohammed, \n",
      "and \n",
      "S. \n",
      "Al-Zawi. \n",
      "Understanding of a convolutional neural network. in 2017 \n",
      "International Conference on Engineering and Technology \n",
      "(ICET). 2017. IEEE. \n",
      "[17] Krizhevsky, A., I. Sutskever, and G.E. Hinton, Imagenet \n",
      "classification with deep convolutional neural networks. \n",
      "Communications of the ACM, 2017. 60(6): p. 84-90. \n",
      "[18] Hasan, T.S., Image compression using discrete wavelet \n",
      "transform and discrete cosine transform. Journal of Applied \n",
      "Sciences Researches, 2017. 13: p. 1-8. \n",
      "[19] Nazir, M., Efficient Facial Expression Classification Using \n",
      "Machine Learning Techniques. 2019, Islamia Collage \n",
      "Peshawar. \n",
      "[20] Gola, J., et al., Objective microstructure classification by \n",
      "support vector machine (SVM) using a combination of \n",
      "morphological parameters and textural features for low \n",
      "carbon steels. Computational Materials Science, 2019. 160: \n",
      "p. 186-196. \n",
      " \n",
      " \n",
      "76\n",
      "\n",
      "Interactive Continual Learning: Fast and Slow Thinking\n",
      "Biqing Qi1,2,4, Xinquan Chen3, Junqi Gao3, Dong Li3, Jianxing Liu1, Ligang Wu1,*\n",
      ", Bowen Zhou1,2,4,*\n",
      "1 Department of Control Science and Engineering, Harbin Institute of Technology,\n",
      "2 Department of Electronic Engineering, Tsinghua University,\n",
      "3 School of Mathematics, Harbin Institute of Technology,\n",
      "4 Frontis.AI, Beijing\n",
      "{qibiqing7,xinquanchen0117,gjunqi97,arvinlee826}@gmail.com,\n",
      "{jx.liu,ligangwu}@hit.edu.cn, {zhoubowen}@tsinghua.edu.cn\n",
      "Abstract\n",
      "Advanced life forms, sustained by the synergistic inter-\n",
      "action of neural cognitive mechanisms, continually acquire\n",
      "and transfer knowledge throughout their lifespan. In con-\n",
      "trast, contemporary machine learning paradigms exhibit\n",
      "limitations in emulating the facets of continual learning\n",
      "(CL). Nonetheless, the emergence of large language mod-\n",
      "els (LLMs) presents promising avenues for realizing CL via\n",
      "interactions with these models. Drawing on Complemen-\n",
      "tary Learning System theory, this paper presents a novel In-\n",
      "teractive Continual Learning (ICL) framework, enabled by\n",
      "collaborative interactions among models of various sizes.\n",
      "Specifically, we assign the ViT model as System1 and mul-\n",
      "timodal LLM as System2.\n",
      "To enable the memory mod-\n",
      "ule to deduce tasks from class information and enhance\n",
      "Set2Set retrieval, we propose the Class-Knowledge-Task\n",
      "Multi-Head Attention (CKT-MHA). Additionally, to improve\n",
      "memory retrieval in System1 through enhanced geomet-\n",
      "ric representation, we introduce the CL-vMF mechanism,\n",
      "based on the von Mises-Fisher (vMF) distribution. Mean-\n",
      "while, we introduce the von Mises-Fisher Outlier Detection\n",
      "and Interaction (vMF-ODI) strategy to identify hard exam-\n",
      "ples, thus enhancing collaboration between System1 and\n",
      "System2 for complex reasoning realization. Comprehensive\n",
      "evaluation of our proposed ICL demonstrates significant re-\n",
      "sistance to forgetting and superior performance relative to\n",
      "existing methods. Code is available at github.com/ICL.\n",
      "1. Introduction\n",
      "Advanced life forms exhibit continual learning (CL) and\n",
      "memory formation, facilitated by neural cognitive inter-\n",
      "actions that enable collaborative knowledge transfer [9,\n",
      "26, 48]. These underlying mechanisms enhance memory\n",
      "*Corresponding authors\n",
      "consolidation and utilization, as well as reasoning abili-\n",
      "ties in advanced life forms [16, 39].\n",
      "However, current\n",
      "machine learning paradigms, particularly neural network-\n",
      "based models, face challenges in achieving CL. Specifi-\n",
      "cally, neural networks learning from evolving data face a\n",
      "risk known as catastrophic forgetting [5], wherein the in-\n",
      "tegration of new knowledge frequently disrupts existing\n",
      "knowledge, resulting in notable performance degradation\n",
      "[5, 17, 50].\n",
      "To tackle this challenge, current CL methods strive to\n",
      "preserve and augment knowledge acquired throughout the\n",
      "learning process [14, 22, 28, 36]. In CL, rehearsal-based\n",
      "methods [32, 36, 40, 44–46], are the most direct strategy.\n",
      "However, these methods often ignore the geometric\n",
      "structure [30] of memory representations and face chal-\n",
      "lenges in open-class settings. Another perspective includes\n",
      "architecture-based methods [21, 22, 31], allocating distinct\n",
      "parameters for knowledge encoding from various tasks.\n",
      "Early studies centered on convolution-based architectures\n",
      "[28]. Recent advancements pivoted towards transformer-\n",
      "based methods like L2P [46], and Dualprompt [45].\n",
      "From the perspective of Complementary Learning Sys-\n",
      "tem (CLS) Theory in neurocognitive science [16], the cur-\n",
      "rent designs of CL frameworks may not be optimal.\n",
      "In\n",
      "a brain-like system, multiple memory modules dynam-\n",
      "ically maintain a balance between stability and plastic-\n",
      "ity, with each module possessing predictive capabilities\n",
      "[24, 33, 35]. However, most advanced CL frameworks lean\n",
      "towards more intuitive systems. Previous CLS-driven meth-\n",
      "ods [25, 28] involve the separation and expansion of param-\n",
      "eters to facilitate the learning of both fast and slow knowl-\n",
      "edge. For instance, DualNet [28] optimizes this process\n",
      "by emphasizing task-specific pattern separation. Similarly,\n",
      "BiMeCo [25] divides model parameters into two distinct\n",
      "components: a short-term memory module and a long-term\n",
      "memory module. However, these methods [25, 28] are lim-\n",
      "ited to a single backbone model. In line with CLS prin-\n",
      "1\n",
      "arXiv:2403.02628v2  [cs.CV]  19 Mar 2024\n",
      "ciples, this underscores the importance of developing an\n",
      "interactive CL framework between models to consistently\n",
      "achieve higher performance levels.\n",
      "Meanwhile, recent advancements in large language mod-\n",
      "els (LLMs), as exemplified by ChatGPT [38] and GPT4\n",
      "[5], have demonstrated remarkable reasoning capabilities.\n",
      "These models can employ chains of thought [47] to engage\n",
      "in complex reasoning, much like System2. Consequently,\n",
      "it raises an interesting question: Can we integrate intuitive\n",
      "models, such as ViT [8] as System1 alongside LLM-based\n",
      "as System2 to establish an interactive framework for CL?\n",
      "To response this question, we reevaluated CL by ex-\n",
      "ploring the interaction between ViT (System1) and Mul-\n",
      "timodal Large Language Model (System2). In alignment\n",
      "with the current CL setting, our focus is on adapting ViT\n",
      "parameters while keeping System2 parameters stable. Sys-\n",
      "tem2 is responsible for handling hard examples and facil-\n",
      "itates collaboration with System1. To enable the contin-\n",
      "ual updating of ViT, we introduced the Class-Knowledge-\n",
      "Task Multi-Head Attention (CKT-MHA) module.\n",
      "CKT-\n",
      "MHA utilizes category features and the knowledge of ViT\n",
      "to aid System1 in acquiring task-related knowledge, facil-\n",
      "itating knowledge retrieval through Class-Task collections.\n",
      "Furthermore, we introduce the CL-vMF mechanism, which\n",
      "employs von Mises-Fisher distribution modeling to improve\n",
      "memory geometry and enhance retrieval distinguishability\n",
      "through an Expectation-Maximization (EM) update strat-\n",
      "egy. This design enables System1 to retain old memory\n",
      "parameters, preventing unnecessary updates and addressing\n",
      "catastrophic forgetting issues. To realize the coordination\n",
      "between System1 and System2 during their reasoning tran-\n",
      "sitions, we introduce the von Mises-Fisher Outlier Detec-\n",
      "tion and Interaction (vMF-ODI) mechanism for assessing\n",
      "sample difficulty. This mechanism is designed to assist the\n",
      "System1 in adaptive identifying hard examples within each\n",
      "batch. Once identified, these hard examples undergo ini-\n",
      "tial inference by System1, and the resulting prediction out-\n",
      "comes serve as background knowledge for System2 to fa-\n",
      "cilitate more intricate reasoning.\n",
      "We conduct experiments on various benchmarks, includ-\n",
      "ing the demanding Imagenet-R, to validate proposed inter-\n",
      "active continual learning (ICL). The results illustrate that\n",
      "ICL significantly mitigates catastrophic forgetting, surpass-\n",
      "ing state-of-the-art methods. Moreover, it maintains consis-\n",
      "tently high accuracy across different task. In summary, our\n",
      "contributions are as follows:\n",
      "• We propose an ICL framework from a novel perspec-\n",
      "tive that emphasizes the interaction between fast intu-\n",
      "itive model (ViT) and slow deliberate model (multimodal\n",
      "LLM), aligning with CLS principles.\n",
      "• We propose the CKT-MHA module to acquire task-\n",
      "related information by leveraging category features and\n",
      "small model knowledge.\n",
      "• We propose the CL-VMF mechanism, an optimization\n",
      "strategy guided by VMF distribution modeling with EM\n",
      "strategy updates to enhance the retrieval of geometric\n",
      "memory representations.\n",
      "• We propose vMF-ODI, a batch-wise retrieval interaction\n",
      "strategy that enables the adaptive identification of hard\n",
      "examples within each batch, fostering collaborative rea-\n",
      "soning between the two systems.\n",
      "2. Related Works\n",
      "We discuss three primary categories of CL methods.\n",
      "Regularization-based methods incorporate regulariza-\n",
      "tion terms into the loss function to mitigate catastrophic\n",
      "forgetting for previously learned tasks.\n",
      "These methods\n",
      "[1, 14, 17, 20, 27, 50], primarily revolve around the de-\n",
      "velopment of metrics for assessing task importance, with\n",
      "additional research efforts dedicated to characterizing the\n",
      "significance of individual features [42]. Nevertheless, these\n",
      "methods tend to exhibit reduced performance when applied\n",
      "to more complex datasets.\n",
      "Rehearsal based methods utilize previous task data to\n",
      "mitigate catastrophic forgetting within limited memory\n",
      "buffers. Reservoir sampling techniques [7, 34], randomly\n",
      "retain a fixed number of old training samples from each\n",
      "training batch. Further, [12] employs coefficient-based co-\n",
      "sine similarity to address sample number imbalances among\n",
      "categories. To better recover past knowledge, GEM [36]\n",
      "constructs individual constraints based on old training sam-\n",
      "ples for each task to ensure no increase in their loss. LOGD\n",
      "[40] decomposes gradients for each task into shared and\n",
      "task-specific components, capitalizing on inter-task infor-\n",
      "mation. CVT [44] explores online CL using external atten-\n",
      "tion strategy.\n",
      "Architecture-based methods aim at assigning indepen-\n",
      "dent parameters for new task data. These methods involve\n",
      "strategies such as parameter allocation, model division, and\n",
      "modular network models [21, 22, 31].\n",
      "Previous studies\n",
      "concentrated on specific convolution-based architectures,\n",
      "with DualNet [28] optimizes memory through separation\n",
      "representations for specific tasks.\n",
      "Recent work focus to\n",
      "transformer-based models. L2P [46] enhances integration\n",
      "of knowledge by treating prompts as optimization param-\n",
      "eters. Furthermore, Dualprompt [45] enhances knowledge\n",
      "memory by constructing dual orthogonal prompt spaces.\n",
      "3. Methodology\n",
      "3.1. Problem Setup\n",
      "A standard paradigm for CL can be defined by a set of\n",
      "task descriptors t ∈T , and the corresponding distribution\n",
      "pt(x, y) for each task.\n",
      "The task-specific dataset Dt :=\n",
      "{(xt\n",
      "i, yt\n",
      "i) | (xt\n",
      "i, yt\n",
      "i) ∼pt, i ∈[Nt]}, where Nt repre-\n",
      "sents the number of samples in the training set of the t-th\n",
      "2\n",
      "Image2language\n",
      "language2Image\n",
      "B.1 Multimodal LLM\n",
      "Query\n",
      "Transformer\n",
      "Chat-LLM\n",
      "System1 Output: [Dog, Cat, Goldfish, ......].\n",
      "System2: From your description, answer which category the picture is belong to:\n",
      "[Dog], [Cat] or [Goldfish]? Please choose one most possible class.\n",
      "System2 Output: It is a [Cat].\n",
      "B.2 Slow Thinking: Inference via Complex Reasoning\n",
      "Embedding Layer\n",
      "Self-Attention\n",
      "input\n",
      "… More layers …\n",
      "tokens\n",
      "output\n",
      "tokens\n",
      "A.1 ViT Model\n",
      "Example Representation\n",
      "Anchor Representation\n",
      "Set2Set\n",
      "Similarity\n",
      "A.3 Fast Thinking: Inference via Similarity Computation\n",
      "C. Metric\n",
      "Evaluation\n",
      "（+）Trustworthiness-enhanced AI tech\n",
      "（+）Active behavior with low inference cost\n",
      "Small Model\n",
      "dominant\n",
      "LLM\n",
      "assists\n",
      "Small Model\n",
      "assist\n",
      "LLM\n",
      "dominant\n",
      "（+）Broaden the boundaries of decision space\n",
      "（+）Uundertake brain-intensive reasoning\n",
      "➕\n",
      "A. System1: Fast Thinking\n",
      "B. System2: Slow Thinking\n",
      "Interactive Continuous Learning（ICL）: From the Fast and Slow Thinking Perspective\n",
      "Confidences\n",
      "Dog\n",
      "Label Words\n",
      "Cat\n",
      "Tiger\n",
      "Easy Example\n",
      "System2: Provide descriptions of the given image in more detail.\n",
      "System2 Output: This picture depicts an orange cat sitting on the ground with a \n",
      "man standing next to it.\n",
      "②\n",
      "③\n",
      "Top-k Predictions\n",
      "with Confidences\n",
      "④Hard Example\n",
      "Value\n",
      "Query\n",
      "Task Level\n",
      "Class Level\n",
      "Knowledge Level\n",
      "①\n",
      "D. Final Predictions with Fast and Slow Thinking\n",
      "Easy\n",
      "Example\n",
      "Hard Example\n",
      "A.2 Continual Structured Memory\n",
      "Figure 1. Comprehensive Training and Testing Illustration. In the training phase: we propose the CKT-MHA unified storage module for\n",
      "System1. And then use memory selection and updates through our CL-VMF mechanism with the EM strategy to optimize CL for the small\n",
      "model ViT. In the inference phase: 1) The process begins by assessing sample complexity using proposed vMF-ODI in System 1. 2) The\n",
      "System1 then swiftly generates inferential predictions. 3) If test samples surpass a complexity threshold, we activate collaborative inference.\n",
      "Specifically, the predictive results from System1 is used as background knowledge to narrow the scope of inference. 4) Subsequently,\n",
      "complex reasoning through the multimodal LLM is applied to achieve the final prediction.\n",
      "task. The dataset is drawn i.i.d. from the sample space\n",
      "X t × Yt ∈X × Y. During the training phase, the train-\n",
      "ing samples are sequentially fed as inputs, following the\n",
      "task descriptors from 0 to |T |.\n",
      "In formulating the ICL\n",
      "framework, we provide definitions for System1 and Sys-\n",
      "tem2, respectively. System1 is instantiated by the model\n",
      "fθ(·) : X 7→Rd parameterized by θ which updates its pa-\n",
      "rameters to θt in the t-th task. Our objective is to deter-\n",
      "mine θ|T | = arg minθ|T | Et∈T E(xt,yt)\n",
      "\u0002\n",
      "ℓ(fθ|T |(xt), yt\n",
      "i)\n",
      "\u0003\n",
      ",\n",
      "ensuring the memory capacity of the System1. Here ℓ(·, yt)\n",
      "represents loss function, with yt ∈Yt. The system1 uti-\n",
      "lizes a memory buffer M and updates θt−1 to θt by using\n",
      "Dt∩M. Furthermore, System2 is instantiated with a Multi-\n",
      "modal LLM represented as gψ to model complex reasoning\n",
      "abilities. To enable collaborative inference, System2 must\n",
      "handle hard samples ˜\n",
      "X that System1 struggles with, maxi-\n",
      "mizing the probability P˜\n",
      "x∈˜\n",
      "X (˜\n",
      "y ̸= arg maxi fθ(˜\n",
      "x)i). This\n",
      "requires the ability of System1 to filter out these samples,\n",
      "thereby improving the inference of System2 by leveraging\n",
      "predicted results from System1 Ifθ for second-stage infer-\n",
      "ence:\n",
      "ˆ\n",
      "˜\n",
      "y = S−1 (gψ (˜\n",
      "x; S (Ifθ(˜\n",
      "x)))) ,\n",
      "(1)\n",
      "here S\n",
      "represents label to prompt operation,\n",
      "while\n",
      "S−1\n",
      "represents\n",
      "its\n",
      "inverse\n",
      "operation.\n",
      "System2\n",
      "is\n",
      "expected to produce an output S(˜\n",
      "y) that minimizes\n",
      "−log pψ (S(˜\n",
      "y) | ˜\n",
      "x; S (Ifθ(˜\n",
      "x))). Next, we will present de-\n",
      "tailed designs for each component of the ICL framework\n",
      "and discuss optimization strategies.\n",
      "3.2. Query and Value Memory for System1\n",
      "In general, deep neural networks implicitly encode data\n",
      "memories within their parameters, and unnecessary param-\n",
      "eter changes can result in memory degradation. The prevail-\n",
      "ing methods to deploying models in downstream tasks in-\n",
      "volves utilizing pre-trained feature extractors and introduc-\n",
      "ing new parameters for adaptation, which has been demon-\n",
      "strated to be effective in CL setting [45, 46]. Neverthe-\n",
      "less, these methods face challenge when the precise num-\n",
      "ber of classes in the downstream task is uncertain. More-\n",
      "over, updating all parameters of the classification head for\n",
      "each new task worsens the issue of forgetting. To address\n",
      "these challenges, we propose separating the model’s param-\n",
      "eters into two distinct groups: value memory parameters,\n",
      "denoted as Z = Z1, Z2, ... with class-specific representa-\n",
      "tions zyt ∈Zt, and query memory parameters represented\n",
      "by θ. This decoupling strategy enhances operational flex-\n",
      "ibility. In simpler terms, we envision Z as a collection of\n",
      "class-specific value memory variables, ensuring that\n",
      "• Value memory parameters Z can be augmented as the\n",
      "number of tasks increases, i.e., there is a memory incre-\n",
      "ment. When training on task t, Z will be updated from\n",
      "{Z1, ..., Zt−1} to {Z1, ..., Zt}.\n",
      "• The relevant value memory for each class is only updated\n",
      "when necessary, i.e., the value memory zyt will not be\n",
      "updated if the input does not contain class yt, thus zyt−1\n",
      "after trained on task t equals to that trained before training\n",
      "on task t.\n",
      "Then we can ensure that the System1 can be equipped with\n",
      "persistent value memory of old data and update query pa-\n",
      "rameters using a portion of the memory buffer as rehearsal\n",
      "samples, thus can effectively handle old data. At the same\n",
      "time, this enables System1 to be unconstrained by a prede-\n",
      "fined number of classes and allows for more flexible mem-\n",
      "ory allocation for new tasks.\n",
      "3\n",
      "Interactive Query and Value Memory with CKT-MHA\n",
      "Firstly, we design such a memory module for System1.\n",
      "Specifically, we propose a Set2Set memory retrieval mech-\n",
      "anism to further enhance memory retrieval stability. This\n",
      "mechanism involves first obtaining class (cls) information\n",
      "via a projector fθc parameterized by θc: ξc = fθc,φ(x) ∈\n",
      "RL×dc, L is the obtained token length. Here we use the\n",
      "pre-trained ViT as image feature extractor fφ, followed by\n",
      "an query interactor fθ, parameterized by θ, introduced for\n",
      "memory matching. This results in fθ,φ = fθ ◦fφ. Sub-\n",
      "sequently, we utilize ξc and pretrained knowledge ξk =\n",
      "fφ(x) ∈RL×dc to construct the Class-Knowledge-Task\n",
      "Multi-Head Attention (CKT-MHA) to capture the task in-\n",
      "formation corresponding to the class:\n",
      "hτ,i = AttentionθSA\n",
      "\u0010\n",
      "ξ[:,Ri]\n",
      "c\n",
      ", ξ[:,Ri]\n",
      "k\n",
      ", fθτ (ξk)[:,Ri]\u0011\n",
      ", (2)\n",
      "ξτ = fθo (Concat [hτ,1, . . . , hτ,Nh]) ∈RL×τ,\n",
      "(3)\n",
      "where Nh is the head number of MHA, Ri\n",
      "= [(i −\n",
      "1) ∗\n",
      "dc\n",
      "Nh , i ∗\n",
      "dc\n",
      "Nh ] is the corresponding attention head\n",
      "interval.\n",
      "Then we combine the class and task fea-\n",
      "tures set to obtain the classification feature:\n",
      "ξ\n",
      "=\n",
      "Concat [ 1\n",
      "L\n",
      "PL\n",
      "l=1 ξl\n",
      "τ, 1\n",
      "L\n",
      "PL\n",
      "l=1 ξl\n",
      "c], where ξl\n",
      "τ denotes the l-\n",
      "th token of ξτ.\n",
      "Consequently, the interactor parameters\n",
      "are denoted as θ = {θc, θτ, θSA, θo}.\n",
      "Finally, we set\n",
      "a group of task-specified value memory vector zτ ∈Zτ\n",
      "and class-specified zc ∈Zc corresponding to the class\n",
      "to form the final value memory variable z for retrieval:\n",
      "z = Concat [zτ, zc]. During the inference stage, the pro-\n",
      "posed task-class Set2Set retrieval is performed from value\n",
      "memory as: ypred = arg maxz∈Z pθ,φ(z|x).\n",
      "With decoupled parameters, we also need to decouple\n",
      "the updates of query memory parameters and value mem-\n",
      "ory parameters to ensure the flexibility of adjusting these\n",
      "two parts of parameters, avoiding unnecessary updates to\n",
      "value memory parameters, which prevents biases in mem-\n",
      "ory variables for future tasks and mitigates catastrophic for-\n",
      "getting. Furthermore, prioritizing value memory parameter\n",
      "optimization guides the optimization of query parameters.\n",
      "Is it possible to achieve such decoupled optimization while\n",
      "ensuring a consistent optimization objective? The inherent\n",
      "nature of EM algorithm provides a framework for meeting\n",
      "these optimization requirements. Specifically, considering\n",
      "the Maximum Likelihood Estimation (MLE) under a clas-\n",
      "sification setting, with the probability of task t denoted as\n",
      "P(t), the objective can be expressed as follows:\n",
      "Minimize\n",
      "θ|T |\n",
      "X\n",
      "t\n",
      "P(t)Ext \u0002\n",
      "−log pθ|T |(yt|xt)\n",
      "\u0003\n",
      ".\n",
      "(4)\n",
      "For the sake of simplicity in the framework, here we only\n",
      "consider the parameters that are being updated.\n",
      "Dur-\n",
      "ing training on task t, the System1 aims to determine\n",
      "θt = arg minθt Ext [−log pθt(yt|xt)]. To achieve this,\n",
      "we model value memory z\n",
      "∈Z as hidden variables,\n",
      "alongside the hidden distribution q(z|x) which satisfies\n",
      "P\n",
      "z∈Z q(z|x) = 1. This allows us to obtain\n",
      "log pθt(yt|xt) = log\n",
      "h pθt(yt,z|xt)\n",
      "q(z|xt)\n",
      "i\n",
      "−log\n",
      "h pθt(z|yt,xt)\n",
      "q(z|xt)\n",
      "i\n",
      ", (5)\n",
      "taking the expectation with respect to q(z|x) on both sides,\n",
      "log pθt(yt|xt) =\n",
      "X\n",
      "z∈Z\n",
      "q(z|xt) log\n",
      "\u0014pθt(yt, z|xt)\n",
      "q(z|xt)\n",
      "\u0015\n",
      ",\n",
      "(6)\n",
      "+\n",
      "X\n",
      "z∈Z\n",
      "q(z|xt) log\n",
      "\u0014\n",
      "q(z|xt)\n",
      "pθt(z|yt, xt)\n",
      "\u0015\n",
      ",\n",
      "(7)\n",
      "the first term of R.H.S is referred to as the Evidence Lower\n",
      "Bound LEL (pθt(yt, z|xt), q(z|xt)), the second term is\n",
      "KL (q(z|xt)∥pθt(z|yt, xt)).\n",
      "Since the KL divergence\n",
      "is non-negative, we can achieve maximum likelihood\n",
      "by iteratively updating Z and θt using the Generalized\n",
      "Expectation-Maximization algorithm (GEM) as follows:\n",
      "Z(i+1) = arg minZ Ext\n",
      "h\n",
      "KL\n",
      "\u0010\n",
      "q(z(i)|xt)∥pθ(i)\n",
      "t (z(i)|yt, xt)\n",
      "\u0011i\n",
      ",\n",
      "(8)\n",
      "θ(i+1)\n",
      "t\n",
      "= arg maxθ Ext\n",
      "h\n",
      "LEL\n",
      "\u0010\n",
      "pθ(i)\n",
      "t (yt, z(i+1)|xt), q(z(i+1)|xt)\n",
      "\u0011i\n",
      ".\n",
      "(9)\n",
      "In the context of supervised learning, we can define z as\n",
      "class-specific zyt ∈Zt. The prior form of q is q(z|xt) =\n",
      "P\n",
      "z∈Z 1(z = zyt). Note that in this case,\n",
      "pθt(z|yt, xt) = pθt(zyt|xt) = pθt(z, yt|xt).\n",
      "(10)\n",
      "Hence, we can express the two distinct objectives in eq.(8)\n",
      "and (9) as a unified objective:\n",
      "θ∗\n",
      "t , Z∗\n",
      "t = arg min\n",
      "θ,Z Ext\n",
      "h\n",
      "−log pθt(zyt|xt)\n",
      "i\n",
      ".\n",
      "(11)\n",
      "When training on a sequence of tasks, if we directly op-\n",
      "timize as in eq.(4) for the current task’s optimal θ∗\n",
      "t\n",
      "=\n",
      "arg minθ Ext [−log pθt(yt|xt)], as is typical, it will in-\n",
      "evitably lead to θ∗\n",
      "t ̸= θ∗\n",
      "t−1, resulting in catastrophic forget-\n",
      "ting. However, our decoupled optimization strategy ensures\n",
      "that the value memory parameters remain optimal for their\n",
      "corresponding tasks, and with the integration of rehearsal,\n",
      "it guarantees that the optimization of query memory param-\n",
      "eters can be consistently guided by such value memory pa-\n",
      "rameters of old tasks, thereby mitigating catastrophic for-\n",
      "getting while continually adapting to new data. Next, we\n",
      "describe how to model pθt(zyt|xt) in eq.(11).\n",
      "3.3. Optimizing Memory via CL-vMF\n",
      "Modelling Posterior with vMF Distribution. To assure\n",
      "value memory vectors more discriminative that facilitate\n",
      "more explicit memory retrieval, necessitating the construc-\n",
      "tion of more separable geometric relationships for them. In-\n",
      "troducing improved geometric relationships for value mem-\n",
      "ory vectors further ensures that the query features of each\n",
      "4\n",
      "class are more separable and closer to the class center when\n",
      "combined with the EM updating strategy. This also aids\n",
      "in more effectively filtering out outliers, laying the ground-\n",
      "work for screening hard samples for System1, as discussed\n",
      "in Section 3.4. Therefore, we opt for the von Mises-Fisher\n",
      "(vMF) distribution, which naturally excels in modeling ge-\n",
      "ometric relationships in high-dimensional spaces [2, 18, 29]\n",
      "and has demonstrated effectiveness in downstream tasks\n",
      "[23, 51]. Its probability density function is as follows:\n",
      "pvMF(x; µ, κ) = Cd(κ) exp(κ ⟨µ, x⟩),\n",
      "(12)\n",
      "where the correlated sample space is defined as {x|x ∈\n",
      "Rd, ∥x∥= 1}.\n",
      "µ ∈Rd represent the mean direc-\n",
      "tion, while κ is the concentration parameter.\n",
      "The con-\n",
      "stant Cd(κ) is only related to κ and d.\n",
      "By setting nor-\n",
      "malized value memory parameter zyt/∥zyt∥to the mean\n",
      "direction, we model the distribution of the normalized fea-\n",
      "ture fθ,φ(xt)/∥fθ,φ(xt)∥for class yt as a vMF distribution\n",
      "with probability density Cd(κ) exp(κ\n",
      "D\n",
      "zyt\n",
      "∥zyt∥,\n",
      "fθ,φ(xt)\n",
      "∥fθ,φ(xt)∥\n",
      "E\n",
      ").\n",
      "By\n",
      "utilizing\n",
      "the\n",
      "constructed\n",
      "probability\n",
      "density\n",
      "for\n",
      "Bayesian discrimination, we can ascertain the form of the\n",
      "posterior probability pθt,φ(zyt|xt) as:\n",
      "pθt,φ(zyt|xt) =\n",
      "exp(κ\n",
      "D\n",
      "zyt\n",
      "∥zyt∥,\n",
      "fθ,φ(xt)\n",
      "∥fθ,φ(xt)∥\n",
      "E\n",
      ")\n",
      "P\n",
      "z′∈Z exp(κ\n",
      "D\n",
      "z′\n",
      "∥z′∥,\n",
      "fθ,φ(xt)\n",
      "∥fθ,φ(xt)∥\n",
      "E\n",
      ").\n",
      "(13)\n",
      "When a sample pair of a new class (xt+1, yt+1) ∈Dt+1 is\n",
      "inputted, a new zyt+1 /\n",
      "∈Z can be assigned to it for mem-\n",
      "ory expansion, thereby avoiding the limitation imposed by\n",
      "the predefined number of classes, i.e., class-free. Combined\n",
      "with the proposed memory module, we introduce CL-vMF,\n",
      "a class-free memory retrieval mechanism that accommo-\n",
      "dates new classes when the total number is unknown, it re-\n",
      "trieves from memories based on the vMF posterior and en-\n",
      "sures that class-specific value memory parameters are only\n",
      "updated when needed.\n",
      "Implementation of CL-vMF.\n",
      "We set the value memory\n",
      "vectors Z as learnable memory embeddings, enabling the\n",
      "model to use them for retrieval using the vMF posterior\n",
      "to ascertain a class of sample.\n",
      "The process resembles\n",
      "constructing a new ”hippocampus” within the pre-trained\n",
      "model by utilizing Z and learning query memory interac-\n",
      "tions via θ to adapt and employ the value memory. In the\n",
      "training phase, upon encountering each new class y, we al-\n",
      "locate a zy, incorporating it into Z. To ensure updating\n",
      "the value memory parameter zy exclusively when the input\n",
      "contains a sample of class y, we introduce the subsequent\n",
      "batchwise vMF loss to maximize likelihood:\n",
      "LvMF(Bt\n",
      "i) =\n",
      "1\n",
      "|Bt\n",
      "i|\n",
      "X\n",
      "(xt,yt)∈Bt\n",
      "i\n",
      "−log\n",
      "h\n",
      "pBt\n",
      "i\n",
      "θ,φ(zyt|xt)\n",
      "i\n",
      ",\n",
      "(14)\n",
      "where Bt\n",
      "i ∈Dt is the i-th input batch of task t and\n",
      "pBt\n",
      "i\n",
      "θ,φ(zyt|xt) =\n",
      "exp(κ\n",
      "D\n",
      "ΠN (zyt),ΠN(ξt)\n",
      "E\n",
      ")\n",
      "P\n",
      "y∈Yt\n",
      "i exp(κ⟨ΠN (zy),ΠN (ξt)⟩), (15)\n",
      "where ξt = fθ,φ(xt), Yt\n",
      "i = Unique({y}(x,y)∈Bt\n",
      "i) is the\n",
      "set of classes that appears in Bt\n",
      "i, ΠN (·) := (·)\n",
      "∥·∥denotes the\n",
      "projection operator. κ is set as a hyperparameter. Hence,\n",
      "our training approach involves alternating between the EM\n",
      "steps based on eq.(8) and eq.(9) throughout the training pro-\n",
      "cess. Nonetheless, when delving deeper into the optimiza-\n",
      "tion process, it becomes imperative to consider the gradient\n",
      "of ΠN (zyt):\n",
      "∇ΠN (zy)\n",
      "h\n",
      "−log\n",
      "\u0010\n",
      "pBt\n",
      "i\n",
      "θ,φ(zyt|xt)\n",
      "\u0011i\n",
      "(16)\n",
      "=\n",
      "\n",
      "\n",
      "\n",
      "h\n",
      "pBt\n",
      "i\n",
      "θ,φ(zyt|xt) −1\n",
      "i\n",
      "κΠN\n",
      "\u0000ξt\u0001\n",
      ",\n",
      "y = yt\n",
      "pBt\n",
      "i\n",
      "θ,φ(zy|xt)κΠN\n",
      "\u0000ξt\u0001\n",
      ",\n",
      "y ̸= yt\n",
      ",\n",
      "thus when pBt\n",
      "i\n",
      "θ,φ(zyt|xt) →1, the gradient for ΠN (zy)\n",
      "(y = yt) will tend to zero, meanwhile, as p approaches 0,\n",
      "∇ΠN (zy)\n",
      "h\n",
      "−log\n",
      "\u0010\n",
      "pBt\n",
      "i\n",
      "θ,φ(zyt|xt)\n",
      "\u0011i\n",
      "(y ̸= yt) will also tend\n",
      "to zero. Likewise, for ΠN\n",
      "\u0000ξt\u0001\n",
      "we have:\n",
      "∇ΠN (ξt)\n",
      "h\n",
      "−log\n",
      "\u0010\n",
      "pBt\n",
      "i\n",
      "θ,φ(zyt|xt)\n",
      "\u0011i\n",
      "(17)\n",
      "= κ\n",
      "\n",
      "X\n",
      "y∈Yt\n",
      "i\n",
      "pBt\n",
      "i\n",
      "θ,φ(zy|xt)ΠN (zy) −ΠN (zyt)\n",
      "\n",
      ".\n",
      "Note that when pBt\n",
      "i\n",
      "θ,φ(zyt|xt) →1,\n",
      "X\n",
      "y∈Yt\n",
      "i\n",
      "pBt\n",
      "i\n",
      "θ,φ(zy|xt)ΠN (zy)→ΠN (zyt).\n",
      "(18)\n",
      "This means that ∇ΠN (ξt)\n",
      "h\n",
      "−log\n",
      "\u0010\n",
      "pBt\n",
      "i\n",
      "θ,φ(zyt|xt)\n",
      "\u0011i\n",
      "will tend\n",
      "to zero.\n",
      "To ensure stable gradients during training and\n",
      "achieve consistent loss reduction, we introduce a gradient\n",
      "stabilization loss\n",
      "ℓδ\n",
      "GS(xt, yt) = ℓδ\n",
      "margin\n",
      "\u0010\n",
      "zyt, ξt\u0011\n",
      "+ P\n",
      "y∈Yt\n",
      "i ,y̸=yt ℓδ\n",
      "margin\n",
      "\u0000−zy, ξt\u0001, (19)\n",
      "where\n",
      "ℓδ\n",
      "margin\n",
      "\u0000zy, ξt\u0001\n",
      "= max(\n",
      "\f\n",
      "\f1 −\n",
      "\n",
      "ΠN (zy) , ΠN\n",
      "\u0000ξt\u0001\u000B\f\n",
      "\f −δ), (20)\n",
      "which provides a constant gradient as compensation, with\n",
      "δ regulating the threshold of gradient compensation. When\n",
      "the loss becomes small, no further compensation is provided\n",
      "due to the instability of the zero point for the absolute value\n",
      "function. Similarly, we incorporate the batchwise gradient\n",
      "stabilization loss Lδ\n",
      "GS(Bt\n",
      "i) =\n",
      "1\n",
      "|Bt\n",
      "i|\n",
      "P\n",
      "(xt,yt)∈Bt\n",
      "i ℓGS(xt, yt)\n",
      "into the objective, leading to the overall loss:\n",
      "L\n",
      "\u0000Bt\n",
      "i\n",
      "\u0001\n",
      "= LvMF\n",
      "\u0000Bt\n",
      "i\n",
      "\u0001\n",
      "+ λLδ\n",
      "GS\n",
      "\u0000Bt\n",
      "i\n",
      "\u0001\n",
      ",\n",
      "(21)\n",
      "here, δ and λ are hyperparameters.\n",
      "5\n",
      "The CL-vMF model possesses several advantageous fea-\n",
      "tures, including incremental value memory and the ability\n",
      "to handle an arbitrary number of classes. As a result, there\n",
      "is no need to retrain the classification head even when the\n",
      "number of classes exceeds the pre-defined limit. Since the\n",
      "value memory parameters Zt are frozen after the comple-\n",
      "tion of task t, i.e. all memories Z1, . . . , Zt−1 remain un-\n",
      "changed both before and after training task t. This guar-\n",
      "antees stable and persistent value memory. Moreover, the\n",
      "storage cost of class value memory parameters is calculated\n",
      "as dzτ × NC + dzc × |T |, where dzc, dzτ and NC repre-\n",
      "sent the dimensions of zτ, zc and the number of classes,\n",
      "respectively. Importantly, this implies that the storage cost\n",
      "of value memory parameters scales linearly with NC.\n",
      "3.4. Collaborative Inference: System1 and System2\n",
      "To align with the CLS, we aim for the ICL framework to\n",
      "activate System2 when System1 fails to perform fast think-\n",
      "ing, i.e. when encounters hard samples. This activation\n",
      "leverages the complex reasoning capabilities of System2\n",
      "to achieve collaborative inference.\n",
      "Specifically, we use\n",
      "MLLM to instantiate gψ. And we propose a hard sample\n",
      "filtering strategy, vMF-ODI, to screen data that challenges\n",
      "the System1. Specifically, we use batchwise normalization\n",
      "to filter outliers, thus identifying hard sample set ˜\n",
      "X:\n",
      "˜\n",
      "Xi = {(˜\n",
      "x, ˜\n",
      "y) ∈Bi | (ν −¯\n",
      "νBi)/σBi < α} ,\n",
      "(22)\n",
      "where ν =\n",
      "⟨fθ,φ(x),z ˆ\n",
      "y⟩\n",
      "∥fθ,φ(x)∥∥z ˆ\n",
      "y∥, ˆ\n",
      "y is the predicted label, νBi =\n",
      "1\n",
      "|Bi|\n",
      "P\n",
      "j∈Bi νj, σ =\n",
      "q\n",
      "1\n",
      "|Bi|−1\n",
      "P\n",
      "j∈Bi(νj −¯\n",
      "ν)2 and α is a\n",
      "detection threshold. For the filtered hard samples, we uti-\n",
      "lize the TopK outputs [fθ,φ(˜\n",
      "x)]K from the System1 to con-\n",
      "struct an inquiry-based language prompt S\n",
      "\u0000[fθ,φ(˜\n",
      "x)]K\u0001\n",
      ".\n",
      "The operation S converts labels to language and prompts\n",
      "the System2 to perform reasoning and rank the results based\n",
      "on the given context, and finally gives the prediction ˆ\n",
      "˜\n",
      "y like\n",
      "we stated in eq. (1). After completing the inference in stage\n",
      "2, if the System2 provides a precise answer, we will use that\n",
      "answer. Otherwise, we will rely on the judgment result from\n",
      "the System1. This interactive scheme suggests the possi-\n",
      "bility of using fine-tuning strategies, such as LoRA [13],\n",
      "to minimize −log pψ\n",
      "\u0000S(y) | x; S\n",
      "\u0000[fθ,φ(x)]K\u0001\u0001\n",
      "with re-\n",
      "spect to ψ. This adjustment would help align the System2\n",
      "with the System1. Consistent with existing CL setups, we\n",
      "focus on the parameter updates of System1. Detailed al-\n",
      "gorithmic description can be found in Appendix A. During\n",
      "the training process, we perform T iterations of EM alter-\n",
      "nating steps for each task, with updates only applied to θ in\n",
      "subsequent updates for that task.\n",
      "4. Experiments\n",
      "4.1. Experimental Setups\n",
      "Following [41, 43], we investigate two common CL se-\n",
      "tups:\n",
      "Task-Incremental Learning (Task IL) and Class-\n",
      "Incremental Learning (Class IL). In Task IL, task identifiers\n",
      "are provided during both the training and testing phases. In\n",
      "contrast, the Class IL protocol assigns task identifiers only\n",
      "during the training phase.\n",
      "During the testing phase, the\n",
      "model faces the challenge of predicting all classes encoun-\n",
      "tered up to that point, making it a more demanding scenario.\n",
      "Datasets. Three datasets are used in our experiments: CI-\n",
      "FAR10 [15], CIFAR100 [15], ImageNet-R [11]. Details of\n",
      "these datasets are in Appendix.\n",
      "Baselines.\n",
      "We combine the proposed ICL framework\n",
      "with the following advanced baselines: Selected rehearsal-\n",
      "based methods: ER [34], A-GEM [6], iCaRL [32], CVT\n",
      "[44], SCoMMER [37], BiMeCo [25]. architecture-based\n",
      "methods:\n",
      "DualNet [28], L2P [46], DualPrompt [45].\n",
      "Additionally, we perform comparisons with well-known\n",
      "regularization-based techniques, namely EWC [14], LwF\n",
      "[19], and SI [50]. Additionally, we assess JOINT, which\n",
      "entails supervised fine-tuning across all task training sets\n",
      "and represents an upper performance limit. We also exam-\n",
      "ine FT-seq, a sequential fine-tuning technique that partially\n",
      "freezes pre-training weights and generally serves as a per-\n",
      "formance lower bound. Both JOINT and FT-seq have two\n",
      "variants, one using ViT [8] and the other using ResNet18\n",
      "[10] as the backbone. Our main focus on comparing these\n",
      "methods with rehearsal- and prompt-based methods.\n",
      "Metrics. We evaluate CL methods in terms of accuracy,\n",
      "following the accuracy definition presented in [3, 43, 45]:\n",
      "AT =\n",
      "1\n",
      "T\n",
      "PT\n",
      "t=1 aT,t, where aT,t represents the testing ac-\n",
      "curacy for task Tt when the model has completed learning\n",
      "task TT .\n",
      "Implementation Details. Following [3, 4, 32, 44, 50], we\n",
      "use RestNet18 [10], ViT [8] as the backbone in System1.\n",
      "For System2, we choose MiniGPT4 [53], Inf-MLLM [52],\n",
      "Pure-MM as the backbone. More details are in Appendix.\n",
      "4.2. Results\n",
      "Extensive experiments are conducted on CIFAR10, CI-\n",
      "FAR100, ImageNet-R. Specifically, we add our ICL to sev-\n",
      "eral state of the arts methods to evaluate its effectiveness.\n",
      "Results of comparisons with State-of-the-Art Methods.\n",
      "The quantitative comparisons are summarized in Tab. 1. In\n",
      "the rehearsal-based method, we experiment with different\n",
      "buffer sizes for comparison. The results consistently show\n",
      "that our method outperforms others. Notably, to better sim-\n",
      "ulate the CL scenario, we restrict the number of epochs to\n",
      "one, allowing the model to encounter the data only once\n",
      "during incremental task learning. This restriction signifi-\n",
      "cantly affects the efficacy of regularization techniques like\n",
      "EWC [14], LwF [19], as well as rehearsal-based methods\n",
      "such as ER [34], A-GEM [6], iCaRL [32] and CVT [44],\n",
      "among others. However, transformer-based approaches like\n",
      "L2P [46] and DualPrompt [45] maintain certain perfor-\n",
      "mance. Additionally, even in the absence of integrating the\n",
      "System2, its incorporation leads to further performance en-\n",
      "6\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100 120 140 160 180 200\n",
      "Number of Class\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "Class IL Accuracy (%)\n",
      "(a) Accuracy over Number of Class (Class IL)\n",
      "L2P\n",
      "DualPrompt\n",
      "DualNet\n",
      "CVT\n",
      "SCoMMER\n",
      "iCarL\n",
      "ICL w/o System2\n",
      "ICL\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100 120 140 160 180 200\n",
      "Number of Class\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "Task IL Accuracy (%)\n",
      "(b) Accuracy over Number of Class (Task IL)\n",
      "L2P\n",
      "DualPrompt\n",
      "DualNet\n",
      "CVT\n",
      "SCoMMER\n",
      "iCarL\n",
      "ICL w/o System2\n",
      "ICL\n",
      "10\n",
      "3\n",
      "10\n",
      "2\n",
      "10\n",
      "1\n",
      "100\n",
      "100\n",
      "10\n",
      "1\n",
      "10\n",
      "2\n",
      "10\n",
      "3\n",
      "74.16\n",
      "74.8\n",
      "78.27\n",
      "76.34\n",
      "74.16\n",
      "74.97\n",
      "78.27\n",
      "76.34\n",
      "74.13\n",
      "74.83\n",
      "78.27\n",
      "76.34\n",
      "73.9\n",
      "(c) Influence of  and  (CIFAR100)\n",
      "10\n",
      "3\n",
      "10\n",
      "2\n",
      "10\n",
      "1\n",
      "100\n",
      "100\n",
      "10\n",
      "1\n",
      "10\n",
      "2\n",
      "10\n",
      "3\n",
      "53.52\n",
      "54.37\n",
      "53.6\n",
      "45.84\n",
      "53.52\n",
      "54.37\n",
      "53.6\n",
      "45.78\n",
      "53.52\n",
      "54.37\n",
      "53.6\n",
      "45.78\n",
      "46.44\n",
      "(d) Influence of  and  (ImageNet-R)\n",
      "CIFAR10\n",
      "CIFAR100\n",
      "Imagenet-R\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "Class-IL Accuracy (%)\n",
      "(e) Influence of \n",
      "=0.5\n",
      "=1\n",
      "=1.5\n",
      "=2\n",
      "CIFAR10\n",
      "CIFAR100\n",
      "Imagenet-R\n",
      "K\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "Class-IL Accuracy (%)\n",
      "(f) Influence of K\n",
      "w/o slow\n",
      "K=2\n",
      "K=3\n",
      "K=4\n",
      "K=5\n",
      "-15\n",
      "-10\n",
      "-5\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "-15\n",
      "-10\n",
      "-5\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "(g) Trained Memories w/o EM\n",
      "Task\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "-15\n",
      "-10\n",
      "-5\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "-15\n",
      "-10\n",
      "-5\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "(h) Trained Memories w/ EM\n",
      "Task\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "46\n",
      "48\n",
      "50\n",
      "52\n",
      "54\n",
      "Figure 2. Further analysis of the proposed ICL. (a) and (b) are forgetting curves of different methods on the ImageNet-R in the Class IL and\n",
      "Task IL scenario respectively. (c) and (d) are the impact of regularization parameters λ and δ on CIFAR100 and ImageNet-R respectively.\n",
      "(e) The impact of concentration κ, is evaluated at values of 0.5, 1, 1.5, and 2, respectively. (f) The impact of the number of category choices\n",
      "K in the prompt, is evaluated at values of 2, 3, 4, and 5 respectively. (g) The impact of training memory without EM strategy. (h) The\n",
      "impact of training memory with EM strategy.\n",
      "Backbone\n",
      "Method Type\n",
      "Memory\n",
      "Method\n",
      "CIFAR10\n",
      "CIFAR100\n",
      "ImageNet-R\n",
      "Buffer\n",
      "Class-IL\n",
      "Task-IL\n",
      "Class-IL\n",
      "Task-IL\n",
      "Class-IL\n",
      "Task-IL\n",
      "ResNet18\n",
      "-\n",
      "-\n",
      "JOINT\n",
      "92.20\n",
      "98.31\n",
      "70.62\n",
      "86.19\n",
      "7.72\n",
      "25.48\n",
      "FT-seq\n",
      "19.62\n",
      "61.02\n",
      "17.58\n",
      "40.46\n",
      "0.59\n",
      "10.82\n",
      "Non-Rehearsal based\n",
      "0\n",
      "EWC[14]\n",
      "17.82\n",
      "83.52\n",
      "7.62\n",
      "55.14\n",
      "1.08\n",
      "21.34\n",
      "LwF[19]\n",
      "18.52\n",
      "84.72\n",
      "8.88\n",
      "61.32\n",
      "1.24\n",
      "45.68\n",
      "SI[50]\n",
      "18.41\n",
      "84.74\n",
      "6.73\n",
      "50.44\n",
      "3.31\n",
      "22.72\n",
      "Rehearsal based\n",
      "200\n",
      "ER[34]\n",
      "44.79\n",
      "91.19\n",
      "21.40\n",
      "61.36\n",
      "1.01\n",
      "15.36\n",
      "A-GEM [6]\n",
      "18.58\n",
      "80.19\n",
      "7.97\n",
      "55.20\n",
      "1.23\n",
      "16.24\n",
      "iCaRL [32]\n",
      "23.80\n",
      "67.82\n",
      "7.31\n",
      "33.10\n",
      "0.81\n",
      "9.20\n",
      "CVT [44]\n",
      "30.74\n",
      "75.92\n",
      "12.09\n",
      "43.14\n",
      "1.60\n",
      "9.01\n",
      "SCoMMER [37]\n",
      "66.35\n",
      "92.66\n",
      "38.89\n",
      "67.62\n",
      "1.73\n",
      "10.65\n",
      "DualNet [28]\n",
      "24.50\n",
      "90.70\n",
      "25.30\n",
      "54.60\n",
      "7.01\n",
      "20.70\n",
      "BiMeCo [25]\n",
      "27.92\n",
      "92.75\n",
      "28.71\n",
      "56.65\n",
      "10.41\n",
      "22.75\n",
      "500/600\n",
      "ER[34]\n",
      "57.74\n",
      "93.61\n",
      "28.02\n",
      "68.23\n",
      "1.27\n",
      "22.84\n",
      "A-GEM [6]\n",
      "24.85\n",
      "84.80\n",
      "8.89\n",
      "51.47\n",
      "1.23\n",
      "19.35\n",
      "iCaRL [32]\n",
      "29.21\n",
      "67.72\n",
      "4.40\n",
      "23.41\n",
      "1.01\n",
      "7.60\n",
      "CVT [44]\n",
      "40.13\n",
      "79.61\n",
      "13.83\n",
      "46.39\n",
      "1.24\n",
      "6.97\n",
      "SCoMMER [37]\n",
      "73.95\n",
      "94.14\n",
      "49.09\n",
      "74.50\n",
      "1.40\n",
      "10.05\n",
      "DualNet [28]\n",
      "35.00\n",
      "91.90\n",
      "34.65\n",
      "62.70\n",
      "8.70\n",
      "20.40\n",
      "BiMeCo [25]\n",
      "38.40\n",
      "93.95\n",
      "38.05\n",
      "64.75\n",
      "12.13\n",
      "22.45\n",
      "ViT\n",
      "-\n",
      "-\n",
      "JOINT\n",
      "97.49\n",
      "99.54\n",
      "87.23\n",
      "97.64\n",
      "74.75\n",
      "83.39\n",
      "FT-seq\n",
      "22.32\n",
      "86.33\n",
      "20.48\n",
      "83.97\n",
      "32.56\n",
      "49.62\n",
      "Non-Rehearsal based\n",
      "0\n",
      "L2P [46]\n",
      "92.22\n",
      "98.99\n",
      "79.68\n",
      "96.24\n",
      "48.68\n",
      "65.38\n",
      "DualPrompt [45]\n",
      "94.43\n",
      "99.32\n",
      "79.98\n",
      "95.92\n",
      "52.20\n",
      "69.22\n",
      "Rehearsal based\n",
      "200\n",
      "L2P [46]\n",
      "67.13\n",
      "96.39\n",
      "65.29\n",
      "92.16\n",
      "36.70\n",
      "55.35\n",
      "DualPrompt [45]\n",
      "70.60\n",
      "97.78\n",
      "65.97\n",
      "92.85\n",
      "38.79\n",
      "59.32\n",
      "ICL w/o System2\n",
      "94.60\n",
      "99.43\n",
      "77.34\n",
      "94.81\n",
      "49.87\n",
      "68.62\n",
      "ICL w MiniGPT4\n",
      "95.34\n",
      "99.56\n",
      "78.28\n",
      "95.70\n",
      "52.46\n",
      "69.87\n",
      "ICL w Inf-MLLM\n",
      "95.42\n",
      "99.56\n",
      "78.55\n",
      "95.83\n",
      "53.20\n",
      "72.96\n",
      "ICL w Pure-MM\n",
      "95.94\n",
      "99.57\n",
      "79.12\n",
      "95.99\n",
      "53.64\n",
      "73.59\n",
      "500/600\n",
      "L2P [46]\n",
      "71.23\n",
      "96.78\n",
      "69.43\n",
      "93.92\n",
      "40.17\n",
      "57.89\n",
      "DualPrompt [45]\n",
      "73.56\n",
      "98.12\n",
      "69.98\n",
      "93.76\n",
      "43.77\n",
      "61.24\n",
      "ICL w/o System2\n",
      "95.54\n",
      "99.52\n",
      "80.67\n",
      "95.24\n",
      "54.65\n",
      "76.02\n",
      "ICL w MiniGPT4\n",
      "96.49\n",
      "99.58\n",
      "81.38\n",
      "95.62\n",
      "55.99\n",
      "79.68\n",
      "ICL w Inf-MLLM\n",
      "96.69\n",
      "99.64\n",
      "82.29\n",
      "96.14\n",
      "57.47\n",
      "81.82\n",
      "ICL w Pure-MM\n",
      "96.83\n",
      "99.68\n",
      "82.43\n",
      "96.35\n",
      "58.18\n",
      "82.64\n",
      "Table 1. Comparison of average incremental accuracy (%) with different continual learning method in two scenarios. Memory Buffer\n",
      "denotes the size of the memory buffer area (0 means no rehearsal is used). Note that when training with the ViT in System1, the buffer size\n",
      "of the Imagenet-R is 600, due to the need to allocate an equal amount of buffer area for each class.\n",
      "hancement. This is demonstrated by a notable increase of\n",
      "over 3% in CL accuracy on the ImageNet-R dataset, ob-\n",
      "served across 10 consecutive splits, with a memory capacity\n",
      "of 600. The introduction of the System2 enhances System1\n",
      "ability to effectively recognize and address previously for-\n",
      "gotten images or information. To ensure fair comparisons,\n",
      "we incorporate a buffer into the L2P [46] and DualPrompt\n",
      "[45] methods, using a fixed strategy akin to CVT [44]. Sub-\n",
      "sequently, we observe that the performance of the L2P [46]\n",
      "and DualPrompt [45] methods declined after integrating the\n",
      "buffer. This decline is likely due to interference from the\n",
      "replayed samples in the buffer, which hampers the train-\n",
      "ing of task-specific prompts and leads to greater forgetting.\n",
      "Results of different task settings.\n",
      "To assess CL strate-\n",
      "gies across varying numbers of data streams, following the\n",
      "protocol outlined in [44, 49]. This method entails divid-\n",
      "ing the ImageNet-R dataset, which consists of 200 classes,\n",
      "into subsets containing 5, 10, and 20 classes each, thereby\n",
      "creating incremental tasks.\n",
      "Tab.\n",
      "2 offers a comprehen-\n",
      "sive analysis of the accuracy achieved by various methods\n",
      "across different task configurations. The results unequiv-\n",
      "ocally establish the significant superiority of our method\n",
      "over both regularization-based and rehearsal-based method\n",
      "in a wide range of incremental division scenarios. Notably,\n",
      "even when the memory capacity is held constant at 200,\n",
      "our method outperforms architecture-based methods such\n",
      "7\n",
      "Memory\n",
      "Method\n",
      "5 splits\n",
      "10 splits\n",
      "20 splits\n",
      "Buffer\n",
      "Class-IL Task-IL Class-IL Task-IL Class-IL Task-IL\n",
      "0\n",
      "EWC[14]\n",
      "1.56\n",
      "11.35\n",
      "1.08\n",
      "21.34\n",
      "8.10\n",
      "12.68\n",
      "LwF[19]\n",
      "1.38\n",
      "14.66\n",
      "1.24\n",
      "45.68\n",
      "0.77\n",
      "14.34\n",
      "SI[50]\n",
      "1.78\n",
      "11.50\n",
      "3.31\n",
      "22.72\n",
      "3.35\n",
      "40.29\n",
      "L2P [46]\n",
      "29.87\n",
      "38.58\n",
      "48.68\n",
      "65.38\n",
      "20.08\n",
      "47.98\n",
      "DualPrompt [45]\n",
      "54.43\n",
      "66.86\n",
      "52.20\n",
      "69.22\n",
      "47.13\n",
      "71.43\n",
      "200\n",
      "ER[34]\n",
      "1.29\n",
      "9.75\n",
      "1.01\n",
      "15.36\n",
      "1.38\n",
      "19.81\n",
      "A-GEM [6]\n",
      "1.30\n",
      "4.23\n",
      "1.23\n",
      "16.24\n",
      "1.34\n",
      "21.78\n",
      "iCaRL [32]\n",
      "0.41\n",
      "3.22\n",
      "0.81\n",
      "9.20\n",
      "0.83\n",
      "15.23\n",
      "CVT [44]\n",
      "1.47\n",
      "5.65\n",
      "1.6\n",
      "9.01\n",
      "1.01\n",
      "13.19\n",
      "SCoMMER [37]\n",
      "0.80\n",
      "3.78\n",
      "1.73\n",
      "10.65\n",
      "0.32\n",
      "10.36\n",
      "DualNet [28]\n",
      "9.32\n",
      "13.14\n",
      "7.01\n",
      "20.70\n",
      "4.92\n",
      "25.53\n",
      "BiMeCo [25]\n",
      "11.18\n",
      "14.27\n",
      "10.41\n",
      "22.75\n",
      "5.86\n",
      "26.33\n",
      "ICL w/o System2 (ours)\n",
      "49.91\n",
      "74.36\n",
      "49.87\n",
      "68.62\n",
      "48.75\n",
      "73.95\n",
      "ICL (ours)\n",
      "54.85\n",
      "75.57\n",
      "52.46\n",
      "69.87\n",
      "49.98\n",
      "74.63\n",
      "500/600\n",
      "ER[34]\n",
      "1.30\n",
      "13.02\n",
      "1.27\n",
      "22.84\n",
      "1.56\n",
      "20.42\n",
      "A-GEM [6]\n",
      "1.31\n",
      "9.55\n",
      "1.23\n",
      "19.35\n",
      "1.58\n",
      "21.89\n",
      "iCaRL [32]\n",
      "0.43\n",
      "3.62\n",
      "1.01\n",
      "7.60\n",
      "1.62\n",
      "14.23\n",
      "CVT [44]\n",
      "1.95\n",
      "5.68\n",
      "1.24\n",
      "6.97\n",
      "1.45\n",
      "15.58\n",
      "SCoMMER [37]\n",
      "0.61\n",
      "3.35\n",
      "1.40\n",
      "10.05\n",
      "0.56\n",
      "12.60\n",
      "DualNet [28]\n",
      "10.03\n",
      "13.44\n",
      "8.70\n",
      "20.40\n",
      "6.40\n",
      "31.80\n",
      "BiMeCo [25]\n",
      "11.89\n",
      "14.57\n",
      "12.13\n",
      "22.45\n",
      "7.34\n",
      "32.73\n",
      "ICL w/o System2 (ours)\n",
      "54.60\n",
      "75.57\n",
      "54.65\n",
      "76.02\n",
      "52.46\n",
      "77.05\n",
      "ICL (ours)\n",
      "56.34\n",
      "78.36\n",
      "55.99\n",
      "79.68\n",
      "53.60\n",
      "80.47\n",
      "Table 2.\n",
      "Comparison under different task number settings on\n",
      "ImageNet-R dataset.\n",
      "as L2P [46] and DualPrompt [45]. Furthermore, the results\n",
      "illustrate a consistent enhancement in our method perfor-\n",
      "mance as the buffer size increases.\n",
      "Results of forgetting curve comparison. To illustrate the\n",
      "forgetting process within each compared methods in the CL\n",
      "data stream, we record the average test accuracy for both the\n",
      "current and preceding tasks upon completing the training of\n",
      "each task. Subsequently, we create a line chart to visualize\n",
      "the change in accuracy with the addition of each task, of-\n",
      "fering a visual representation of the forgetting process. Fig.\n",
      "2(a) and Fig. 2(b) provide clear illustrations that, as new\n",
      "tasks are introduced, most methods exhibit a decline in per-\n",
      "formance. However, our method consistently outperforms\n",
      "these methods in terms of accuracy at every stage.\n",
      "4.3. Ablation Study\n",
      "Analysis of hyperparameters λ and margin δ.\n",
      "Fig. 2(c) and Fig. 2(d) depict the impact of parameters λ\n",
      "and δ on our method performance, using the CIFAR100 and\n",
      "ImageNet-R datasets, with memory sizes set at 500 and 600,\n",
      "respectively. An analysis of the heat maps reveals that our\n",
      "method’s sensitivity to λ varies between datasets. Specifi-\n",
      "cally, CIFAR100 demonstrates optimal performance when\n",
      "λ = 0.1, while ImageNet-R maintains consistent perfor-\n",
      "mance with λ values within the range [0.001, 0.01, 0.1]. In\n",
      "contrast, variations in δ do not result in substantial accu-\n",
      "racy differences between the datasets. This suggests that\n",
      "the margin regularization term has only a marginal impact\n",
      "on overall model performance. Importantly, when λ = 0\n",
      "and δ = 1 (as shown in the bottom), the absence of the\n",
      "regularization term leads to diminished performance, un-\n",
      "derscoring the value of incorporating margin regularization\n",
      "to enhance the model’s effectiveness.\n",
      "Analysis of concentration parameter Selection. We con-\n",
      "duct an investigation into the influence of the concentration\n",
      "parameter κ on model performance. Fig. 2(e) indicates that\n",
      "there is no significant variation in performance across dif-\n",
      "ferent values of κ for the CIFAR10 dataset. However, for\n",
      "datasets such as CIFAR100 and ImageNet-R, it becomes\n",
      "important to estimate the concentration in advance to en-\n",
      "sure a more accurate modeling of the concentration. Conse-\n",
      "quently, these datasets demonstrate a noticeable sensitivity\n",
      "to the concentration parameter.\n",
      "Impact of Top-k results in collaboration inference. To in-\n",
      "vestigate the influence of the number of prompt categories\n",
      "K on the reasoning process of System2, a series of experi-\n",
      "ments are conducted using different values of K on multiple\n",
      "datasets, as depicted in Fig. 2(f). The results clearly indi-\n",
      "cate that memory vectors possessing well-defined geomet-\n",
      "ric structures facilitate stable memory retrieval, thereby pre-\n",
      "venting System1 from deviating significantly from the cen-\n",
      "tral data point during the reasoning phase. Consequently,\n",
      "when a smaller number of top-k choices are provided, it\n",
      "can ensure almost guaranteed inclusion of the correct cate-\n",
      "gories. Conversely, when K is larger, there is an increased\n",
      "tendency for erroneous category information to be included\n",
      "in the prompt, further intensifying the potential for con-\n",
      "fusion in System2. Fig. 2(f) also presents the accuracy\n",
      "achieved without System2 across various datasets (blue hor-\n",
      "izontal line in the figure), demonstrating the performance\n",
      "enhancement by the incorporation of System2.\n",
      "Impact of separate query and value memory optimiza-\n",
      "tion. To assess the query-value separation strategy impact\n",
      "on stable value memory modeling in persistent scenarios,\n",
      "we compare its use (Fig. 2(g)) and absence (Fig. 2(h)).\n",
      "Visualizing the value memory via tsne reduction reveals\n",
      "that query-value parameter optimization can create more\n",
      "focused value memory modeling, enhancing task discrim-\n",
      "ination. The corresponding results consistently validate the\n",
      "effectiveness of our proposed CL-vMF mechanism.\n",
      "5. Conclusion\n",
      "In our paper, we introduced ICL, a groundbreaking con-\n",
      "tinual learning (CL) paradigm inspired by Complementary\n",
      "Learning System theory in neurocognitive science.\n",
      "ICL\n",
      "combines ViT with an interactive query and value memory\n",
      "module powered by CKT-MHA, enhancing the efficiency of\n",
      "fast thinking (System1). Additionally, it leverages our CL-\n",
      "vMF mechanism to improve memory representation dis-\n",
      "tinction. ICL also integrates multi-modal Large Language\n",
      "Models (System2) with System1 for advanced reasoning,\n",
      "dynamically modulated by hard examples detected through\n",
      "our VMF-ODI strategy. Our experiments confirmed the ef-\n",
      "fectiveness of our framework in reducing forgetting, sur-\n",
      "passing contemporary state-of-the-art methods.\n",
      "Acknowledgement\n",
      "This\n",
      "work\n",
      "was\n",
      "supported\n",
      "in\n",
      "part by the National Key R&D Program of China (No.\n",
      "2023YFC3305102).\n",
      "We extend our gratitude to the\n",
      "anonymous reviewers for their insightful feedback, which\n",
      "has greatly contributed to the improvement of this paper.\n",
      "8\n",
      "References\n",
      "[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny,\n",
      "Marcus Rohrbach, and Tinne Tuytelaars.\n",
      "Memory aware\n",
      "synapses: Learning what (not) to forget.\n",
      "In Proceedings\n",
      "of the European Conference on Computer Vision (ECCV),\n",
      "pages 139–154, 2018. 2\n",
      "[2] Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh,\n",
      "and Suvrit Sra.\n",
      "Clustering on the unit hypersphere using\n",
      "von mises-fisher distributions.\n",
      "J. Mach. Learn. Res., 6:\n",
      "1345–1382, 2005. 5\n",
      "[3] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide\n",
      "Abati, and Simone Calderara.\n",
      "Dark experience for gen-\n",
      "eral continual learning: a strong, simple baseline. Advances\n",
      "in neural information processing systems, 33:15920–15930,\n",
      "2020. 6, 1\n",
      "[4] Pietro Buzzega, Matteo Boschini, Angelo Porrello, and Si-\n",
      "mone Calderara.\n",
      "Rethinking experience replay: a bag of\n",
      "tricks for continual learning. In 2020 25th International Con-\n",
      "ference on Pattern Recognition (ICPR), pages 2180–2187.\n",
      "IEEE, 2021. 6, 1\n",
      "[5] Yihan Cao, Siyu Li, Yixin Liu, Zhiling Yan, Yutong Dai,\n",
      "Philip S Yu, and Lichao Sun. A comprehensive survey of\n",
      "ai-generated content (aigc): A history of generative ai from\n",
      "gan to chatgpt. arXiv preprint arXiv:2303.04226, 2023. 1, 2\n",
      "[6] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach,\n",
      "and Mohamed Elhoseiny. Efficient lifelong learning with a-\n",
      "gem. arXiv preprint arXiv:1812.00420, 2018. 6, 7, 8\n",
      "[7] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny,\n",
      "Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS\n",
      "Torr, and Marc’Aurelio Ranzato. On tiny episodic memo-\n",
      "ries in continual learning. arXiv preprint arXiv:1902.10486,\n",
      "2019. 2\n",
      "[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\n",
      "Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\n",
      "Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\n",
      "vain Gelly, et al. An image is worth 16x16 words: Trans-\n",
      "formers for image recognition at scale.\n",
      "arXiv preprint\n",
      "arXiv:2010.11929, 2020. 2, 6\n",
      "[9] Jonathan St BT Evans. In two minds: dual-process accounts\n",
      "of reasoning. Trends in cognitive sciences, 7(10):454–459,\n",
      "2003. 1\n",
      "[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n",
      "Deep residual learning for image recognition. In Proceed-\n",
      "ings of the IEEE conference on computer vision and pattern\n",
      "recognition, pages 770–778, 2016. 6\n",
      "[11] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kada-\n",
      "vath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu,\n",
      "Samyak Parajuli, Mike Guo, et al. The many faces of robust-\n",
      "ness: A critical analysis of out-of-distribution generalization.\n",
      "In Proceedings of the IEEE/CVF International Conference\n",
      "on Computer Vision, pages 8340–8349, 2021. 6\n",
      "[12] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and\n",
      "Dahua Lin. Learning a unified classifier incrementally via\n",
      "rebalancing. In Proceedings of the IEEE/CVF conference on\n",
      "Computer Vision and Pattern Recognition, pages 831–839,\n",
      "2019. 2\n",
      "[13] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,\n",
      "Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-\n",
      "rank adaptation of large language models. In International\n",
      "Conference on Learning Representations, 2021. 6\n",
      "[14] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel\n",
      "Veness, Guillaume Desjardins, Andrei A Rusu, Kieran\n",
      "Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-\n",
      "Barwinska, et al. Overcoming catastrophic forgetting in neu-\n",
      "ral networks. Proceedings of the National Academy of Sci-\n",
      "ences, 114(13):3521–3526, 2017. 1, 2, 6, 7, 8\n",
      "[15] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple\n",
      "layers of features from tiny images. 2009. 6\n",
      "[16] Dharshan Kumaran, Demis Hassabis, and James L McClel-\n",
      "land.\n",
      "What learning systems do intelligent agents need?\n",
      "complementary learning systems theory updated. Trends in\n",
      "cognitive sciences, 20(7):512–534, 2016. 1\n",
      "[17] Janghyeon Lee, Hyeong Gwon Hong, Donggyu Joo, and\n",
      "Junmo Kim. Continual learning with extended kronecker-\n",
      "factored approximate curvature.\n",
      "In Proceedings of the\n",
      "IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition, pages 9001–9010, 2020. 1, 2\n",
      "[18] Omer Levy, Yoav Goldberg, and Ido Dagan. Improving dis-\n",
      "tributional similarity with lessons learned from word embed-\n",
      "dings. Transactions of the Association for Computational\n",
      "Linguistics, 3:211–225, 2015. 5\n",
      "[19] Zhizhong Li and Derek Hoiem. Learning without forgetting.\n",
      "IEEE transactions on pattern analysis and machine intelli-\n",
      "gence, 40(12):2935–2947, 2017. 6, 7, 8\n",
      "[20] Xialei Liu, Marc Masana, Luis Herranz, Joost Van de Weijer,\n",
      "Antonio M Lopez, and Andrew D Bagdanov. Rotate your\n",
      "networks: Better weight consolidation and less catastrophic\n",
      "forgetting. In 2018 24th International Conference on Pattern\n",
      "Recognition (ICPR), pages 2262–2268. IEEE, 2018. 2\n",
      "[21] Arun Mallya and Svetlana Lazebnik. Packnet: Adding mul-\n",
      "tiple tasks to a single network by iterative pruning. In Pro-\n",
      "ceedings of the IEEE conference on Computer Vision and\n",
      "Pattern Recognition, pages 7765–7773, 2018. 1, 2\n",
      "[22] Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggy-\n",
      "back: Adapting a single network to multiple tasks by learn-\n",
      "ing to mask weights. In Proceedings of the European con-\n",
      "ference on computer vision (ECCV), pages 67–82, 2018. 1,\n",
      "2\n",
      "[23] Yu Meng, Jiaming Shen, Chao Zhang, and Jiawei Han.\n",
      "Weakly-supervised hierarchical text classification.\n",
      "In The\n",
      "Thirty-Third AAAI Conference on Artificial Intelligence,\n",
      "AAAI 2019, The Thirty-First Innovative Applications of Arti-\n",
      "ficial Intelligence Conference, IAAI 2019, The Ninth AAAI\n",
      "Symposium on Educational Advances in Artificial Intelli-\n",
      "gence, EAAI 2019, Honolulu, Hawaii, USA, January 27 -\n",
      "February 1, 2019, pages 6826–6833, 2019. 5\n",
      "[24] Martial Mermillod, Aur´\n",
      "elia Bugaiska, and Patrick Bonin.\n",
      "The stability-plasticity dilemma: Investigating the contin-\n",
      "uum from catastrophic forgetting to age-limited learning ef-\n",
      "fects, 2013. 1\n",
      "[25] Xing Nie, Shixiong Xu, Xiyan Liu, Gaofeng Meng, Chunlei\n",
      "Huo, and Shiming Xiang. Bilateral memory consolidation\n",
      "for continual learning. In Proceedings of the IEEE/CVF Con-\n",
      "9\n",
      "ference on Computer Vision and Pattern Recognition, pages\n",
      "16026–16035, 2023. 1, 6, 7, 8\n",
      "[26] Randall C O’Reilly and Kenneth A Norman. Hippocampal\n",
      "and neocortical contributions to memory: Advances in the\n",
      "complementary learning systems framework. Trends in cog-\n",
      "nitive sciences, 6(12):505–510, 2002. 1\n",
      "[27] Dongmin Park, Seokil Hong, Bohyung Han, and Kyoung Mu\n",
      "Lee.\n",
      "Continual learning by asymmetric loss approxima-\n",
      "tion with single-side overestimation. In Proceedings of the\n",
      "IEEE/CVF International Conference on Computer Vision,\n",
      "pages 3335–3344, 2019. 2\n",
      "[28] Quang Pham, Chenghao Liu, and Steven Hoi. Dualnet: Con-\n",
      "tinual learning, fast and slow. Advances in Neural Informa-\n",
      "tion Processing Systems, 34:16131–16144, 2021. 1, 2, 6, 7,\n",
      "8\n",
      "[29] Biqing Qi, Bowen Zhou, Weinan Zhang, Jianxing Liu, and\n",
      "Ligang Wu.\n",
      "Improving robustness of intent detection un-\n",
      "der adversarial attacks: A geometric constraint perspective.\n",
      "IEEE transactions on neural networks and learning systems,\n",
      "PP, 2023. 5\n",
      "[30] Biqing Qi, Bowen Zhou, Weinan Zhang, Jianxing Liu, and\n",
      "Ligang Wu.\n",
      "Improving robustness of intent detection un-\n",
      "der adversarial attacks: A geometric constraint perspective.\n",
      "IEEE Transactions on Neural Networks and Learning Sys-\n",
      "tems, 2023. 1\n",
      "[31] Qi Qin, Wenpeng Hu, Han Peng, Dongyan Zhao, and Bing\n",
      "Liu. Bns: Building network structures dynamically for con-\n",
      "tinual learning. Advances in Neural Information Processing\n",
      "Systems, 34:20608–20620, 2021. 1, 2\n",
      "[32] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg\n",
      "Sperl, and Christoph H Lampert. icarl: Incremental classifier\n",
      "and representation learning. In Proceedings of the IEEE con-\n",
      "ference on Computer Vision and Pattern Recognition, pages\n",
      "2001–2010, 2017. 1, 6, 7, 8\n",
      "[33] Blake A Richards and Paul W Frankland. The persistence\n",
      "and transience of memory. Neuron, 94(6):1071–1084, 2017.\n",
      "1\n",
      "[34] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu,\n",
      "Irina Rish, Yuhai Tu, and Gerald Tesauro. Learning to learn\n",
      "without forgetting by maximizing transfer and minimizing\n",
      "interference. arXiv preprint arXiv:1810.11910, 2018. 2, 6,\n",
      "7, 8\n",
      "[35] Tom´\n",
      "as J Ryan and Paul W Frankland. Forgetting as a form\n",
      "of adaptive engram cell plasticity. Nature Reviews Neuro-\n",
      "science, 23(3):173–186, 2022. 1\n",
      "[36] Gobinda Saha, Isha Garg, and Kaushik Roy.\n",
      "Gradient\n",
      "projection memory for continual learning.\n",
      "arXiv preprint\n",
      "arXiv:2103.09762, 2021. 1, 2\n",
      "[37] Fahad Sarfraz, Elahe Arani, and Bahram Zonooz. Sparse\n",
      "coding in a dual memory system for lifelong learning. In\n",
      "Proceedings of the AAAI Conference on Artificial Intelli-\n",
      "gence, pages 9714–9722, 2023. 6, 7, 8\n",
      "[38] John Schulman, Barret Zoph, Christina Kim, Jacob Hilton,\n",
      "Jacob Menick, Jiayi Weng, Juan Felipe Ceron Uribe, Liam\n",
      "Fedus, Luke Metz, Michael Pokorny, et al. Chatgpt: Opti-\n",
      "mizing language models for dialogue. OpenAI blog, 2022.\n",
      "2\n",
      "[39] Advani Sun, Weinan et al. Organizing memories for gener-\n",
      "alization in complementary learning systems. Nature neuro-\n",
      "science, 26(8):1438–1448, 2023. 1\n",
      "[40] Shixiang Tang, Dapeng Chen, Jinguo Zhu, Shijie Yu, and\n",
      "Wanli Ouyang.\n",
      "Layerwise optimization by gradient de-\n",
      "composition for continual learning. In Proceedings of the\n",
      "IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition, pages 9634–9643, 2021. 1, 2\n",
      "[41] Gido M Van de Ven and Andreas S Tolias. Three scenar-\n",
      "ios for continual learning. arXiv preprint arXiv:1904.07734,\n",
      "2019. 6\n",
      "[42] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A\n",
      "comprehensive survey of continual learning: Theory, method\n",
      "and application. arXiv preprint arXiv:2302.00487, 2023. 2\n",
      "[43] Zhen Wang, Liu Liu, Yiqun Duan, Yajing Kong, and\n",
      "Dacheng Tao. Continual learning with lifelong vision trans-\n",
      "former.\n",
      "In Proceedings of the IEEE/CVF Conference on\n",
      "Computer Vision and Pattern Recognition (CVPR), pages\n",
      "171–181, 2022. 6\n",
      "[44] Zhen Wang, Liu Liu, Yajing Kong, Jiaxian Guo, and\n",
      "Dacheng Tao. Online continual learning with contrastive vi-\n",
      "sion transformer. In ECCV, 2022. 1, 2, 6, 7, 8\n",
      "[45] Zifeng Wang, Zhang, et al. Dualprompt: Complementary\n",
      "prompting for rehearsal-free continual learning. In ECCV,\n",
      "pages 631–648, 2022. 1, 2, 3, 6, 7, 8\n",
      "[46] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang,\n",
      "Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer\n",
      "Dy, and Tomas Pfister.\n",
      "Learning to prompt for continual\n",
      "learning. In Proceedings of the IEEE/CVF Conference on\n",
      "Computer Vision and Pattern Recognition, pages 139–149,\n",
      "2022. 1, 2, 3, 6, 7, 8\n",
      "[47] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\n",
      "Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\n",
      "Chain-of-thought prompting elicits reasoning in large lan-\n",
      "guage models. Advances in Neural Information Processing\n",
      "Systems, 35:24824–24837, 2022. 2\n",
      "[48] Gordon Winocur, Morris Moscovitch, and Melanie Sekeres.\n",
      "Memory consolidation or transformation: context manipu-\n",
      "lation and hippocampal representations of memory. Nature\n",
      "neuroscience, 10(5):555–557, 2007. 1\n",
      "[49] Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynam-\n",
      "ically expandable representation for class incremental learn-\n",
      "ing. In Proceedings of the IEEE/CVF Conference on Com-\n",
      "puter Vision and Pattern Recognition, pages 3014–3023,\n",
      "2021. 7\n",
      "[50] Friedemann Zenke, Ben Poole, and Surya Ganguli. Contin-\n",
      "ual learning through synaptic intelligence. In International\n",
      "Conference on Machine Learning, pages 3987–3995. PMLR,\n",
      "2017. 1, 2, 6, 7, 8\n",
      "[51] Chao Zhang, Liyuan Liu, Dongming Lei, Quan Yuan, Hon-\n",
      "glei Zhuang, Timothy Hanratty, and Jiawei Han.\n",
      "Triove-\n",
      "cevent: Embedding-based online local event detection in\n",
      "geo-tagged tweet streams. pages 595–604, 2017. 5\n",
      "[52] Qiang Zhou, Zhibin Wang, Wei Chu, Yinghui Xu, Hao Li,\n",
      "and Yuan Qi.\n",
      "Infmllm: A unified framework for visual-\n",
      "language tasks, 2023. 6\n",
      "[53] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\n",
      "hamed Elhoseiny.\n",
      "Minigpt-4: Enhancing vision-language\n",
      "10\n",
      "understanding with advanced large language models. arXiv\n",
      "preprint arXiv:2304.10592, 2023. 6, 2\n",
      "11\n",
      "Interactive Continual Learning: Fast and Slow Thinking\n",
      "Supplementary Material\n",
      "6. Algorithms of ICL\n",
      "We formalize the algorithms for the training and inference\n",
      "stages of ICL, as shown in Algorithm 1 and 2. Here, we set\n",
      "the detection threshold α as the upper 20th percentile of the\n",
      "standard normal distribution, which is −0.842.\n",
      "Algorithm 1 Training Stage of ICL\n",
      "Require: The parameters of the image feature extractor of\n",
      "the pre-trained ViT φ, the memory buffer M with size\n",
      "|M|, the parameters of the query memory θ, the CL\n",
      "training dataset Dt, 1 ≤t ≤|T| with nt batches\n",
      "1: Initialize value memory parameters Z = ∅and memory\n",
      "buffer M = ∅.\n",
      "2: for t = 1 ←|T| do\n",
      "3:\n",
      "for i = 1 ←nt do\n",
      "4:\n",
      "if t > 1 then\n",
      "5:\n",
      "Randomly sample a batch ˜\n",
      "Bt\n",
      "i from M\n",
      "6:\n",
      "Bt\n",
      "i = Bt\n",
      "i ∪˜\n",
      "Bt\n",
      "i\n",
      "7:\n",
      "end if\n",
      "8:\n",
      "if ∃(xt, yt) ∈Bt\n",
      "i s.t. no z ∈Z matches yt then\n",
      "9:\n",
      "Add zyt = Concat[zt, zyt] into Z.\n",
      "10:\n",
      "end if\n",
      "11:\n",
      "E-step: Update value memory parameters Z on\n",
      "L(Bt\n",
      "i)\n",
      "12:\n",
      "M-step: Update query memory parameters θ on\n",
      "L(Bt\n",
      "i)\n",
      "13:\n",
      "Update memory buffer M\n",
      "14:\n",
      "end for\n",
      "15:\n",
      "Freeze the memory parameters of classes in task t\n",
      "16: end for\n",
      "7. Datasets Settings\n",
      "CIFAR-10 comprises 10 classes, each with 50,000 train-\n",
      "ing and 10,000 test color images. CIFAR-100 includes 100\n",
      "classes, offering 500 training and 100 testing images per\n",
      "class. ImageNet-R, an extension of the ImageNet dataset,\n",
      "possess 200 classes. It contains a total of 30,000 images, of\n",
      "which 20% were allocated as the test set.\n",
      "CIFAR-10 was divided into five tasks, two classes allo-\n",
      "cated to each task. CIFAR-100 was divided into ten tasks,\n",
      "each task with ten classes. Similarly, ImageNet-R was or-\n",
      "ganized into ten tasks, with each task containing 20 classes.\n",
      "Input images were resized to 224 × 224 and normalized to\n",
      "the range [0, 1]. ICL was compared against both represen-\n",
      "tative baselines and state-of-the-art methods across diverse\n",
      "buffer sizes and datasets.\n",
      "Algorithm 2 Inference Stage of ICL\n",
      "Require: The image feature extractor in pre-trained ViT\n",
      "fφ, the trained query and value memory fθ, Z, the test\n",
      "dataset D with n batches.\n",
      "1: for i = 1 ←n do\n",
      "2:\n",
      "for j ←|Bi| do\n",
      "3:\n",
      "ˆ\n",
      "yj = arg maxy pBt\n",
      "i\n",
      "θ,φ(zyi|x)\n",
      "4:\n",
      "end for\n",
      "5:\n",
      "˜\n",
      "Xi = {(˜\n",
      "x, ˜\n",
      "y) ∈Bi | (ν −¯\n",
      "νBi)/σBi < α}\n",
      "6:\n",
      "if ˜\n",
      "Xi = ∅then\n",
      "7:\n",
      "Return the prediction results of System 1\n",
      "8:\n",
      "else\n",
      "9:\n",
      "Using System 2, perform inference on x ∈˜\n",
      "Xi by\n",
      "combining the top-K output of System 1. Retrieve\n",
      "the result of the exact answer and combine it with\n",
      "the remaining predictions from System 1 before\n",
      "returning.\n",
      "10:\n",
      "end if\n",
      "11: end for\n",
      "8. Implementation Details\n",
      "To ensure a fair comparison between methods, we carried\n",
      "out uniform resizing of the images to dimensions of 224 ×\n",
      "224 and applied image normalization. Following the set-\n",
      "tings of [3, 4, 32, 44], we adopted 10 batch size and 1 epoch\n",
      "for all methods during training, utilizing cross-entropy as\n",
      "the classification loss. For the L2P[46], DualPrompt[45]\n",
      "approaches, we followed the implementation details of the\n",
      "original paper and employed ViT as the backbone network,\n",
      "while ResNet18 served as the backbone network for the\n",
      "remaining methods. We meticulously reproduced the out-\n",
      "comes by adhering to the original implementation and set-\n",
      "tings. We have set up separate Adam optimizers with a con-\n",
      "stant learning rate of 1e−4 for the query and value memory\n",
      "parameters.\n",
      "9. Inference with System 1\n",
      "We conducted a comparison by directly applying rehearsal-\n",
      "based fine-tuning with a same-sized buffer, using only the\n",
      "pretrained ViT with a trainable classification head on each\n",
      "dataset.\n",
      "The results, as shown in Tab.\n",
      "3, were signifi-\n",
      "cantly lower than those obtained using only System 1. This\n",
      "stark contrast serves as strong evidence that both ViT and\n",
      "MiniGPT-4 have not undergone pretraining on the three\n",
      "datasets and highlights the effectiveness of our proposed\n",
      "method.\n",
      "1\n",
      "Memory\n",
      "Method\n",
      "CIFAR10\n",
      "CIFAR100\n",
      "ImageNet-R\n",
      "Buffer\n",
      "Class-IL\n",
      "Task-IL\n",
      "Class-IL\n",
      "Task-IL\n",
      "Class-IL\n",
      "Task-IL\n",
      "200\n",
      "ViT Finetune\n",
      "33.15\n",
      "96.00\n",
      "32.60\n",
      "91.50\n",
      "20.88\n",
      "64.45\n",
      "ICL w/o System2\n",
      "94.60\n",
      "99.43\n",
      "77.34\n",
      "94.81\n",
      "49.87\n",
      "68.62\n",
      "500/600\n",
      "ViT Finetune\n",
      "62.65\n",
      "97.15\n",
      "45.30\n",
      "92.80\n",
      "33.26\n",
      "75.50\n",
      "ICL w/o System2\n",
      "95.54\n",
      "99.52\n",
      "80.67\n",
      "95.24\n",
      "54.65\n",
      "76.02\n",
      "Table 3. Comparison of incremental accuracy (%). Vit Finetune\n",
      "represents the basic rehearsal method using Vit as the backbone.\n",
      "10. Inference with System 2\n",
      "In order to validate the mutually beneficial interaction be-\n",
      "tween systems, we conduct experiments using the pre-\n",
      "trained MiniGPT4 [53] to perform inference on the test sets\n",
      "of CIFAR-10, CIFAR-100, and ImageNet-R. MiniGPT4\n",
      "loads the official 7B pre-trained parameters, and the prompt\n",
      "used by MiniGPT4 is the same as System2. Since System 1\n",
      "does not provide a topk option, we provided all categories to\n",
      "MiniGPT4, allowing it to select a category for image clas-\n",
      "sification based on the image description. Tab. 4 presents\n",
      "the accuracy of reasoning, error rate, and proportion of no\n",
      "exact response (i.e. there is not only one class in the given\n",
      "classes is returned or no response).\n",
      "Dataset\n",
      "Accuracy\n",
      "Error\n",
      "No Response\n",
      "Total\n",
      "CIFAR-10\n",
      "9.53\n",
      "15.04\n",
      "75.43\n",
      "10000\n",
      "CIFAR-100\n",
      "2.45\n",
      "14.53\n",
      "83.02\n",
      "10000\n",
      "ImageNet-R\n",
      "2.67\n",
      "10.33\n",
      "87.00\n",
      "6000\n",
      "Table 4.\n",
      "MiniGPT4’s inference accuracy, error rate, and pro-\n",
      "portion of no exact response on the CIFAR-10, CIFAR-100, and\n",
      "ImageNet-R test sets. The number of responses for each test set\n",
      "are reported in the last column.\n",
      "The results presented in the table indicate that over\n",
      "75% of the images fed in MiniGPT4, when applied to the\n",
      "CIFAR-10 dataset, fail to return a specific class to which the\n",
      "image belongs. And when faced with the CIFAR-100 and\n",
      "ImageNet-R datasets, MiniGPT4 with prompt that includes\n",
      "a larger number of classes, encounters increased difficulty\n",
      "in making accurate selections. Among the images that were\n",
      "returned with specific class information, over two-thirds\n",
      "were misclassified. These experimental results demonstrate\n",
      "that relying solely on MiniGPT4 for image classification\n",
      "tasks yields poor performance. Nevertheless, when System\n",
      "1 offers the top-K option, incorporating MiniGPT4 as Sys-\n",
      "tem2 enhances the image classification task and improves\n",
      "the final accuracy. This finding demonstrates that the in-\n",
      "teractive inference between System1 and System2 enables\n",
      "mutual promotion and improvement.\n",
      "The limitations of MiniGPT-4 restricted the performance\n",
      "enhancement of System 2. To address these concerns, we\n",
      "chose more advanced MLLMs as System 2. As depicted in\n",
      "Tab. 1, there was a notable 3-4% improvement, especially\n",
      "on the challenging ImageNet-R dataset.\n",
      "2\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'score'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[119], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m inputs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquestion\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhat is the Deep learning system?\u001B[39m\u001B[38;5;124m\"\u001B[39m}\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m app\u001B[38;5;241m.\u001B[39mstream(inputs):\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m output\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m      4\u001B[0m         pprint(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFinished running: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages/langgraph/pregel/__init__.py:983\u001B[0m, in \u001B[0;36mPregel.stream\u001B[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001B[0m\n\u001B[1;32m    980\u001B[0m         \u001B[38;5;28;01mdel\u001B[39;00m fut, task\n\u001B[1;32m    982\u001B[0m \u001B[38;5;66;03m# panic on failure or timeout\u001B[39;00m\n\u001B[0;32m--> 983\u001B[0m \u001B[43m_panic_or_proceed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdone\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minflight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstep\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    984\u001B[0m \u001B[38;5;66;03m# don't keep futures around in memory longer than needed\u001B[39;00m\n\u001B[1;32m    985\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m done, inflight, futures\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1537\u001B[0m, in \u001B[0;36m_panic_or_proceed\u001B[0;34m(done, inflight, step)\u001B[0m\n\u001B[1;32m   1535\u001B[0m             inflight\u001B[38;5;241m.\u001B[39mpop()\u001B[38;5;241m.\u001B[39mcancel()\n\u001B[1;32m   1536\u001B[0m         \u001B[38;5;66;03m# raise the exception\u001B[39;00m\n\u001B[0;32m-> 1537\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[1;32m   1539\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inflight:\n\u001B[1;32m   1540\u001B[0m     \u001B[38;5;66;03m# if we got here means we timed out\u001B[39;00m\n\u001B[1;32m   1541\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m inflight:\n\u001B[1;32m   1542\u001B[0m         \u001B[38;5;66;03m# cancel all pending tasks\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.14/lib/python3.10/concurrent/futures/thread.py:58\u001B[0m, in \u001B[0;36m_WorkItem.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfuture\u001B[38;5;241m.\u001B[39mset_exception(exc)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages/langgraph/pregel/retry.py:72\u001B[0m, in \u001B[0;36mrun_with_retry\u001B[0;34m(task, retry_policy)\u001B[0m\n\u001B[1;32m     70\u001B[0m task\u001B[38;5;241m.\u001B[39mwrites\u001B[38;5;241m.\u001B[39mclear()\n\u001B[1;32m     71\u001B[0m \u001B[38;5;66;03m# run the task\u001B[39;00m\n\u001B[0;32m---> 72\u001B[0m \u001B[43mtask\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mproc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;66;03m# if successful, end\u001B[39;00m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages/langchain_core/runnables/base.py:2502\u001B[0m, in \u001B[0;36mRunnableSequence.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m   2498\u001B[0m config \u001B[38;5;241m=\u001B[39m patch_config(\n\u001B[1;32m   2499\u001B[0m     config, callbacks\u001B[38;5;241m=\u001B[39mrun_manager\u001B[38;5;241m.\u001B[39mget_child(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mseq:step:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2500\u001B[0m )\n\u001B[1;32m   2501\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 2502\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mstep\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2503\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2504\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m step\u001B[38;5;241m.\u001B[39minvoke(\u001B[38;5;28minput\u001B[39m, config)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.14/envs/ai-advanced/lib/python3.10/site-packages/langgraph/utils.py:95\u001B[0m, in \u001B[0;36mRunnableCallable.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m accepts_config(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunc):\n\u001B[1;32m     94\u001B[0m         kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfig\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m config\n\u001B[0;32m---> 95\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(ret, Runnable) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrecurse:\n\u001B[1;32m     97\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ret\u001B[38;5;241m.\u001B[39minvoke(\u001B[38;5;28minput\u001B[39m, config)\n",
      "Cell \u001B[0;32mIn[109], line 99\u001B[0m, in \u001B[0;36mgrade_documents\u001B[0;34m(state)\u001B[0m\n\u001B[1;32m     94\u001B[0m score \u001B[38;5;241m=\u001B[39m retrieval_grader\u001B[38;5;241m.\u001B[39minvoke(\n\u001B[1;32m     95\u001B[0m     {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquestion\u001B[39m\u001B[38;5;124m\"\u001B[39m: question, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdocument\u001B[39m\u001B[38;5;124m\"\u001B[39m: d\u001B[38;5;241m.\u001B[39mpage_content}\n\u001B[1;32m     96\u001B[0m )\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28mprint\u001B[39m(d\u001B[38;5;241m.\u001B[39mpage_content)\n\u001B[0;32m---> 99\u001B[0m grade \u001B[38;5;241m=\u001B[39m \u001B[43mscore\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mscore\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;66;03m# Document relevant\u001B[39;00m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m grade\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myes\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[0;31mKeyError\u001B[0m: 'score'"
     ]
    }
   ],
   "execution_count": 119
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ea3728d53cc34f28"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
